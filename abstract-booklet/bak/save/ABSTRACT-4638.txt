A recent trend of big data problems is to develop computationally efficient inferential methods that embed computational thinking into uncertainty quantification. In this talk I will introduce two new classes of nonparametric testing that scale well with large datasets. One class is based on randomized sketches which can be implemented in one computer, while the other class requires parallel computing. Our theoretical contribution is to characterize the minimal computational cost that is needed to achieve testing optimality. Optimal estimation is a byproduct. The proposed methods are examined by simulated and real datasets.