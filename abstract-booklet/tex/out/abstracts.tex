
\subsection*{Morning sessions}

\subsubsection*{1. New Vistas in Statistics with Applications}

\begin{itemize}
\item \textbf{Aleksey Polunchenko}, Binghamton University \\
``Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts Diffusion with Constant Positive Drift'' \\
Aleksey Polunchenko


We consider the first exit time of a Shiryaev-Roberts diffusion with constant positive drift from the interval $[0,A]$ where $A>0$. We show that the moment generating function (Laplace transform) of a suitably standardized version of the first exit time converges to that of the unit-mean exponential distribution as $A\to+\infty$. The proof is explicit in that the moment generating function of the first exit time is first expressed analytically and in a closed form, and then the desired limit as $A\to+\infty$ is evaluated directly. The result is of importance in the area of quickest change-point detection, and its discrete-time counterpart has been previously established - although in a different manner - by Pollak and Tartakovsky (2009).

\item \textbf{Emmanuel Yashchin}, IBM Research \\
``Alarm prioritization in Early Warning Systems'' \\
Emmanuel Yashchin


In complex manufacturing and business operations, early warning systems (EWSs) ensure timely detection of unfavorable trends. Such systems can be deployed so that they act as search engines, analyzing available data at time points that are either pre-specified or determined based on process information. A round of analysis typically encompasses a large number of data streams that are governed by an even larger set of statistical parameters. Careful design of monitoring procedures ensures a low rate of false alarms. To ensure efficient utilization of personnel, it is important that these alarms are properly prioritized. We discuss methods and statistics relevant in the process of alarm prioritization, and their use in the field of integrated circuit manufacturing. 

\end{itemize}

\subsubsection*{3. Space-Time Statistical Solutions at Ibm Research}

\begin{itemize}
\item \textbf{Julie Novak}, IBM Research \\
``Statistical Challenges of Large-Scale Revenue Forecasting'' \\
Julie Novak, Stefa Etchegaray Garcia, Yasuo Amemiya


Large-scale businesses need to have a clear vision of how well they expect to perform within all their different units. This information will directly impact managerial decisions that will in turn affect the future health of the company. In this talk, we focus on the statistical challenges that occur when implementing our revenue forecasting methodology on a weekly basis within a large business. We must provide reasonably accurate forecasts for all the geography/division combinations, which have fundamentally different revenue trends and patterns over time. Our method must be robust to “oddities”, such as typos in the input or unusual behavior in the data. In addition, our forecasts must be stable over weeks, without sacrificing on accuracy. We describe the statistical methods used to maintain an efficient and effective operational solution. 

\item \textbf{Yasuo Amemiya}, IBM T. J. Watson Research Center \\
``Spatio-Temporal Analysis for System Management'' \\
Yasuo Amemiya, Youngdeok Hwang


IBM has been providing analytics-based solutions to various large-scale problems relevant for business, government, and society.  A goal of such a project is to manage a large physical system effectively based on analysis of various measurements taken over space and time.  Statistical analysis methods and ideas are essential part of the overall solution development.  In particular, new types of spatio-temporal analysis methods are needed.  In this talk, some of large system management projects at IBM Research are described, and the development of appropriate spatio-temporal analysis methods is discussed.

\end{itemize}

\subsubsection*{5. Big Data}

\begin{itemize}
\item \textbf{Li Ma}, Duke University \\
``Fisher exact scanning for dependency'' \\
Li Ma, Jialiang Mao


We introduce a method—called Fisher exact scanning (FES)—for testing and identifying variable dependency that generalizes Fisher’s exact test on 2-by-2 contingency tables to R-by-C contingency tables and continuous sample spaces. FES proceeds through scanning over the sample space using windows in the form of 2-by-2 tables of various sizes, and on each window completing a Fisher’s exact test. Based on a factorization of Fisher’s multivariate hypergeometric (MHG) likelihood into the product of the univariate hypergeometric likelihoods, we show that there exists a coarse-to-fine, sequential generative representation for the MHG model in the form of a Bayesian network, which in turn implies the mutual independence (up to deviation due to discreteness) among the Fisher’s exact tests completed under FES. This allows an exact characterization of the joint null distribution of the p-values and gives rise to an effective inference recipe through simple multiple testing procedures such as Sidak and Bonferroni corrections, eliminating the need for resampling. In addition, FES can characterize dependency through reporting significant windows after multiple testing control. The computational complexity of FES scales linearly with the sample size, which along with the avoidance of resampling makes it ideal for analyzing massive data sets. We use extensive numerical studies to illustrate the work of FES and compare it to several state-of-the-art methods for testing dependency in both statistical and computational performance. Finally, we apply FES to analyzing a microbiome data set and further investigate its relationship with other popular dependency metrics in that context.

\end{itemize}

\subsubsection*{6. Bayesian Applications in High-Dimensional and Multivariate Modeling}

\begin{itemize}
\item \textbf{Seongho Song}, University of Cincinnati \\
``Bayesian Multivariate Gamma-Frailty Cox Model for Clustered Current Status Data”'' \\
Negar Jaberansari, Dipak K. Dey and Seongho Song


Biomedical data analysis plays a key role in today's medicine. Multivariate current status data is a common type of Biomedical data which gives rise to two main challenges in data analysis. First, all event times are censored, making censoring times the only indicator of event occurrence. Second, an unobserved heterogeneity caused by clusters of units or individuals is probable. To address these issues, mixed Cox proportional hazard model with random block frailty has been used. Here, we consider a Bayesian multivariate Gamma-frailty Cox model and augment the likelihood with respect to random frailties and a set of Poisson latent variables. We also introduce a novel MCMC algorithm by employing two diff erent cumulative baseline hazard function structures: a transformed mixture of incomplete Beta distributions and a linear combination of monotone integrated splines. Through several simulations, we show that our methodology achieves competitive results. We also compare the performance of the two baseline hazard functions using model selection criteria such as AIC and DIC. Finally, we apply the model to a bivariate current status cataract dataset and investigate the e ffect of various risk factors on the occurrence of
cataracts.

\item \textbf{Gyuhyeong Goh}, Kansas State University \\
``Bayesian variable selection using marginal posterior consistency'' \\
Gyuhyeong Goh, Dipak K. Dey


Due to recent technological advancements, high-dimensional data are frequently involved in many areas of science. When an extreme large number of possible predictors are under consideration for the data, marginal likelihood estimation provides an effective way to reduce the high-dimensionality. However, the marginal likelihood-based approach ignores simultaneous influence of predictors and often leads to misidentification of relevant predictors. In this paper, we propose a new variable selection procedure for accounting for the joint influence of important predictors. We use marginal posterior distributions to incorporate all possible predictor effects into the variable selection procedure. Some theoretical properties of the proposed method are investigated. A simulation study demonstrates that our Bayesian approach provides better variable selection performance than existing marginal likelihood methods.

\end{itemize}

\subsubsection*{9. Statistical Approaches in Modeling and Incorporating Dependence}

\begin{itemize}
\item \textbf{Mengyu Xu}, University of Central Florida \\
``Pearson's Chi-squared statistics: approximation theory and beyond'' \\
Mengyu Xu, Danna Zhang, Wei Biao Wu


We establish a Chi-squared approximation theory for Pearson's Chi-squared statistics by using a high-
dimensional central limit theorem for quadratic forms of random vectors. Our high-
dimensional central limit theorem or invariance principle is proved under Lyapunov-
type conditions that involve a delicate interplay between the dimension p, the sample
size n and the moment condition. To obtain cutoff values of our tests, we introduce a plug-in Gaussian multiplier calibration method and normalized consistency, a new matrix convergence criterion. Based on our modified Chi-squared statistic, we propose the concept of adjusted degrees of freedom. We develop a Cramer-von Mises type test for testing distributions of high dimensional data and develop an approximation theory based on our invariance principle.

\end{itemize}

\subsubsection*{10. Survival Analysis}

\begin{itemize}
\item \textbf{Sangwook Kang}, Yonsei University, Korea \\
``Accelerated failure time modeling via nonparametric infinite scale mixtures'' \\
Byungtae Seo, Sangwook Kang


A semiparamtric accelerated failure time (AFT) model resembles the usual linear regression model with the response variable being the logarithm of failure times while the random error term is left unspecified. Thus, it is more flexible than parametric AFT models that
assume parametric distributions for the random error term. Estimation for model parameters is typically done through a rank-based procedure, in which the intercept term cannot be directly esitmated. This requires a separate estimation procedure for the intercept, which often leads to unstable estimates. For better estimation of the intercept essential in estimating mean failure times or survival functions, we propose to employ a mixture model approach. To leave the model as 
flexible as possible, we consider nonparametric infinite scale mixtures of normal distributions. An expectation-maximization (EM) method is used to estimate model parameters. Finite sample properties of the proposed estimators are investigated via
an extensive stimulation study. The proposed estimators are illustrated using a real data analysis.

\item \textbf{Daniel Nevo}, Harvard University \\
``Calibration models for survival analysis with interval-censored exposure or treatment starting time'' \\
Daniel Nevo, Tsuyoshi Hamada, Shuji Ogino and Molin Wang


We consider the association of a time-dependent binary treatment or exposure with time-to-event under the proportional hazard model. The exposure value is assumed zero at the beginning of the study and may change to one at any time point. The value of the exposure is observed only in certain time points, and thus its exact value is unknown for some participants, in each risk set. We are motivated by the assessment of post colorectal cancer diagnosis aspiring taking and survival. Naïve and popular methods are potentially biased, especially when the exposure is measured at a small number of time points. We present a class of calibration models that fit a distribution for the time to exposure starting time. Estimates obtained from these models are then incorporated in the partial likelihood in a natural way. We derive asymptotic theory for these methods. Our methodology allows for inclusion of further baseline covariates affecting the initiation time of the exposure of interest.  Certain bias is expected from our methods when the exposure effect is large, and we provide a less-biased alternative using a risk set calibration approach. 

\item \textbf{Bella Vakulenko-Lagun}, Harvard University \\
``Cox regression for right-truncated data'' \\
Bella Vakulenko-Lagun, Rebecca Betensky, Micha Mandel


Right-truncated survival data arise when observations are sampled retrospectively and
only those who had experienced the event of interest prior to some sampling time are
included in a sample. As a result, the obtained sample is biased, since those who survive
longer have lower probability to be selected.
If the interest is in the nonparametric estimation of the lifetime distribution from right-
truncated data, then this task can be approached by reversing time and transforming the
problem of right-truncation into a well-developed problem of estimation under left truncation. However, when the goal is to explain survival by some covariates, it is unclear how
to interpret results from the reverse time analysis in terms of the forward time effects of
covariates. Other existing methods for the Cox regression under right truncation, although
can be used for testing covariate effect, suffer from an identifiability problem in estimation
or are computationally intensive.
The proposed approach based on the Inverse-Probability-Weighting (IPW) estimating
equations does not have an identifiability problem, it works in a forward time so that covariate effects can be interpreted as usual, it performs better than existing methods for both purposes of testing and estimation, and it is easily implemented using standard software.
Methods are compared in simulations and through an application to real data.

\end{itemize}

\subsubsection*{11. Extremes}

\begin{itemize}
\item \textbf{John Nolan}, American University \\
``mvevd: an R package for extreme value distributions'' \\
Anne-Laure Fougeres, Cecile Mercadier, John Nolan


We present a new way to estimate multivariate extreme value distributions (MVEVD) from data using max projections.
The approach works in any dimension, though computation time increases quickly as dimension increases.
The procedure requires tools from computational geometry and multivariate integration techniques.
An R package mevd is being developed to implement the method for several semi-parametric classes of MEVDs:
discrete angular measure, generalized logistic, piecewise linear angular measures, and Dirichlet mixture models.

\item \textbf{Karthyek Murthy}, Columbia University in the City of New York \\
``Distributionally robust extreme value analysis'' \\
Jose Blanchet, Karthyek Murthy


Typical studies in distributional robustness involve computing worst-case bounds for the quantity of interest (such as expected risk, probability of default, etc.) regardless of the probability distribution used, as long as the distribution lies within a prescribed tolerance (measured in terms of a probabilistic divergence like KL divergence) from a suitable baseline model. 

With this practice of computing worst-case bounds over probabilistic distance based neighborhoods gaining popularity, we go beyond the standard choice of KL divergence to study the role of putative model uncertainty in the context of estimation of tail probabilities or quantiles. In particular, we precisely   characterise  the worst-case extreme value index in order to answer “how heavy the tails of neighboring distributions can be?”. This study seeks to understand the qualitative properties of probabilistic distance based neighborhoods in order to guide the selection of model ambiguity regions for estimating extreme quantiles.  

\item \textbf{Tiandong Wang}, Cornell University \\
``Asymptotic normality of in- and out-degree counts in a preferential attachment model'' \\
Tiandong Wang, Sidney Resnick


Preferential attachment in a directed scale-free graph is an often used paradigm for modeling the evolution of social networks. Social network data is usually given in a format allowing recovery of the number of nodes with in-degree i and out-degree j. Assuming a model with preferential attachment, formal statistical procedures for estimation can be based on such data summaries. Anticipating the statistical need for such node-based methods, we prove asymptotic normality of the node counts. Our approach is based on a martingale construction and a martingale central limit theorem.

\end{itemize}

\subsubsection*{12. Feinberg Memorial Session: Bayesian Statistics with Applications}

\begin{itemize}
\item \textbf{DIlli Bhatta}, University of South Carolina Upstate \\
``A Bayesian Test of Independence in a Two-Way  Contingency Table  Under Two-Stage Cluster Sampling with Covariates'' \\
DIlli Bhatta, Balgobin Nandram, Joseph Sedransk


We consider a Bayesian approach for the test of independence to study the association between two categorical variables with covariates using data from a two-stage cluster sampling design. Under this approach, we convert the cluster sample with covariates into an equivalent simple random sample without covariates which provides a surrogate of the original sample. Then, this surrogate sample is used to compute the Bayes factor to make an inference about independence.
We apply our methodology to the data from the Trend in International Mathematics and Science Study (2007) for fourth grade U.S. students to assess the association between the mathematics and science scores represented as categorical variables. 
We show that if there is strong association between two categorical variables, there is no significant difference between the tests with and without the covariates. We also performed a simulation study to further understand the effect of covariates in various situations. We found that in borderline cases (moderate association between the two categorical variables) there are noticeable differences in the test with and without covariates. 

\end{itemize}

\subsection*{Afternoon sessions}

\subsubsection*{3. Application of Statistical/Predictive Modeling in Health Related Industry}

\begin{itemize}
\item \textbf{Zhaonan Sun}, IBM Research \\
``Exploiting Convolutional Neural Network for Risk Prediction with Medical Feature Embedding'' \\
Zhengping Che, Yu Cheng, Zhaonan Sun, Yan Liu


The widespread availability of electronic health records (EHRs) promises to usher in the era of personalized medicine. However, the problem of extracting useful clinical representations from longitudinal EHR data remains challenging, owing to the heterogeneous, longitudinally irregular, noisy and incomplete nature of such data.

In this talk, we will focus on the problems of high dimensionality and temporality. We explore deep neural network models with learned medical feature embedding to deal with these issues. Specifically, we use a multi-layer convolutional neural network (CNN) to parameterize the model and is thus able to capture complex non-linear longitudinal evolution of EHRs. To account for high dimensionality, we extended the word2vec model and use the embedded medical features in the CNN model. Experiments on real-world EHR data demonstrate the effectivenes of the proposed method. 

\item \textbf{Xiaoyu Jia}, Icahn School of Medicine at Mount Sinai \\
``Opportunities and Challenges in Leveraging Results from Analysis of National Cancer Data Base (NCDB): A Call for Improvement in Quality and Reproducibility'' \\
Xiaoyu Jia, Madhu Mazumdar


Use of national registry databases for performing comparative effectiveness research is on rise as they present wonderful opportunity for answering questions about the effectiveness of treatments in the adjuvant or neoadjuvant setting and the associations of patient or tumor characteristics with treatment selection and clinical outcomes.  Advanced statistical regression models are available for finding answers to these questions. However, lack of analytic code sharing detailing how the data was manipulated, absence of details about modeling techniques and variables used, and in-sufficient validation of modeling present challenges in understanding how the results could be applicable to one’s practice. STROBE and RECORD guidelines are published to guide the design and reporting of observational studies (OS), particularly, those based on routinely collected health care data. Despite emerging evidence that use of reporting guidelines improve quality of reporting, many journals have still not adopted these guidelines and even when adopted, have not mandated their use. We focus our attention to published OS based on National Cancer Data Base (NCDB), a commonly used database in oncology research, and Journal of Clinical Oncology (JCO), a high-impact journal, and a recent time frame of Jan 2015 to March of 2017.  We checked the 16 publications found to assess how well they followed the 22 criteria specified by STROBE/RECORD guideline. Best-practices especially those recommended by RECORD on code sharing and model validation were followed at low-moderate rate in the range 0-25%. We call for JCO and others to adopt reporting guidelines seriously.  Despite availability of large sample size and a rich array of clinical variables, the results based on NCDB will not progress to clinical practice until we could improve the quality and reproducibility of these studies. 

\item \textbf{Victoria Gamerman}, Boehringer-Ingelheim Pharmaceuticals, Inc. \\
``Focusing on patients: going beyond RCTs'' \\
Steven Edelman, Matthew Capehorn, Anne Belton, Susan Down, Aus Alzaid, Friederike Nagel, Jisoo Lee, William H. Polonsky


Type 2 diabetes (T2D) presents challenges both for physicians, who often have limited time and resources, and for patients, who can experience psychological and behavioural issues. Effective communication between physicians and patients, especially during the early phases of T2D treatment, may lead to improvements in patient self-care and outcomes, which is important considering the clinical benefits associated with achieving good glycaemic control early in the course of T2D.

IntroDia™ – a large cross-national survey in 26 countries – has investigated physician-patient communication during early T2D treatment. The survey was designed in partnership with the International Diabetes Federation and a multidisciplinary advisory board. Around 17,000 participants (physicians and patients with T2D) were surveyed using validated scales and novel questionnaires; these assessed physician-patient communication both at diagnosis and at first prescription of additional oral medication, as well as patient-reported outcomes. 

Overall, findings from IntroDia™ suggest that patient-physician communication at diagnosis of T2D and at add-on may be enhanced by physicians using more collaborative and encouraging – and fewer discouraging – conversation elements, and this may contribute to patients subsequently experiencing greater well-being and managing the disease more effectively.

Methodologies and results from the survey will be highlighted.

\end{itemize}

\subsubsection*{4. Biopharmaceutical Session}

\begin{itemize}
\item \textbf{Joseph C. Cappelleri}, Pfizer Inc \\
``Meta-Analysis of Safety Data in Clinical Trials'' \\
Joseph C Cappelleri


Meta-analyses of clinical trial safety data have risen in importance beyond regulatory submissions. During drug development, pharmaceutical sponsors need to recognize safety signals early and adjust the development program accordingly, so as to facilitate the assessment of causality. Once a medicinal product is marketed, sponsors add post-approval clinical trial data to the body of information to help understand existing safety concerns or those that arise from other post-approval data sources, such as spontaneous reports. The situation becomes more involved when interest centers on a network comparison of multiple active treatments.  This presentation highlights some of the major issues considered in meta-analysis of safety data – such as sparse events, reporting quality, and limited study duration – and identifies gaps requiring special attention.   

\item \textbf{QIQI DENG}, Boehringer Ingelheim \\
``Choosing timing and boundary for futility analysis based on cost-effective assessment'' \\
QiQi Deng, Xiaoqi Lu


When a futility analyses is included in a trial, it’s important to choose the right timing for the interim analysis as well as an appropriate futility boundary, so that the trial is likely to be stopped when the interim data suggests a reasonable treatment effect dose not likely exist. This idea is appealing from an ethical point of view since it may reduce the exposure of patients to ineffective treatments, and from a financial point of view since phase III trials are usually the most significant investment in drug development. However, the design may become inefficient if timing and boundary of futility analysis are not chosen carefully. In this presentation, we will use cost-effectiveness analysis to assess the performance of different futility rules, and introduce a graphical tool to guide the selection of design parameters. In addition, we will discuss how prior information/belief of the treatment effect and other factors may influence the futility decision.

\item \textbf{Abidemi Adeniji}, EMD Serono \\
``Estimation of Discrete Survival Function Through the Modeling of Diagnostic Accuracy for Mismeasured Outcome Data'' \\
Hee-Koung Joeng, Abidemi K. Adeniji, Naitee Ting and Ming-Hui Chen


Standard survival methods are inappropriate for mismeasured outcomes. Previous research has shown that outcome misclassification can bias estimation of the survival function. We develop methods to accurately estimate the survival function when the diagnostic tool used to measure the outcome of disease is not perfectly sensitive and specific. Since the diagnostic tool used to measure disease outcome is not the gold standard, the true or error-free outcomes are latent, they cannot be observed. Our method uses the negative predictive value (NPV) and the positive predictive values (PPV) of the diagnostic tool to construct a bridge between the error-prone outcomes and the true
outcomes. We formulate an exact relationship between the true (latent) survival function and the observed (error-prone) survival function as a formulation of time-varying NPV and PPV. We specify models for the NPV and PPV that depend only on parameters that can be easily estimated from a fraction of the observed data. Furthermore, we conduct an in depth study to accurately estimate the latent survival function based on the assumption that the biology that underlies the disease process follows a stochastic process. We further examine the performance of our method by applying it to the VIRAHEP-C data.

\item \textbf{Bushi Wang}, Boehringer Ingelheim \\
``How to Evaluate Type II Error Rate with Multiple Endpoints'' \\
Bushi Wang; Naitee Ting


The FDA draft guidance on multiple endpoints in clinical trials (January 2017) pointed out the regulatory concern of the type II error rate inflation with multiple endpoints. Many of the statistical adjustment to control the type I error rate for multiplicity decrease the study power because they lowered the alpha level used for each of the individual endpoint. The use of co-primary endpoints does not require multiplicity adjustment for type I error but will also increase the type II error rate and decrease study power. In this presentation, I provide a few detailed steps on how to evaluate sample size based on the objective of the clinical study and the selected multiplicity adjustment to control type I error. Analytic forms of power for individual endpoint hypothesis can be derived for most commonly seen scenarios. Simulation can be also easily set up. Optimal sample size is possible by fine tune the individual power for each endpoint with different effect size assumptions.

\end{itemize}

\subsubsection*{5. Complex Data/Network Modeling}

\begin{itemize}
\item \textbf{Xuan Bi}, Yale University \\
``Genome-Wide Mediation Analysis of Psychiatric and Cognitive Traits through Imaging Phenotypes'' \\
Xuan Bi, Liuqing Yang, Tengfei Li, Baisong Wang, Hongtu Zhu, Heping Zhang


Heritability is well documented for psychiatric disorders and cognitive abilities which are, however, complex, involving both genetic and environmental factors. Hence, it remains challenging to discover which and how genetic variations contribute to such complex traits. In this article, we propose to use mediation analysis to bridge this gap, where neuroimaging phenotypes were utilized as intermediate variables. The Philadelphia Neurodevelopmental Cohort was investigated using genome-wide association studies (GWAS) and mediation analyses. Specifically, 951 participants were included with age ranging from 8 to 21 years. Two hundred and four neuroimaging measures were extracted from structural magnetic resonance imaging scans. GWAS were conducted for each measure to evaluate the SNP-based heritability. Furthermore, mediation analyses were employed to understand the mechanisms in which genetic variants have influence on pathological behaviors implicitly through neuroimaging phenotypes. Our analyses found, rs10494561, located within NMNAT2, to be associated with the severity of the prodromal symptoms of psychosis implicitly, mediated through the volume of the left hemisphere of the superior frontal region. Another SNP rs2285351 was found in the IFT122 gene that may be potentially associated with human spatial orientation ability through the area of the left hemisphere of the isthmuscingulate region.

\item \textbf{henry7}, mhlinderm \\
``hlin'' \\
mlkj


;L;

\end{itemize}

\subsubsection*{6. Spatial Analysis of Public Health Data}

\begin{itemize}
\item \textbf{Joshua Warren}, Yale University \\
``A Spatial Method to Estimate Local Vaccine Uptake Using Administrative Records'' \\
Joshua Warren, Esra Kurum, Daniel Weinberger


It is necessary to quantify the level of vaccine uptake among a population of interest in order to determine if the introduced vaccine has the desired beneficial impact on human health.  A number of data sources and methods are available to obtain this information at aggregated spatial levels for many vaccines.  However, obtaining an accurate assessment of uptake at more localized spatial scales can be a difficult task due to limitations of regularly collected administrative data.  Vaccine recipients often live in one region while being vaccinated in another, thereby complicating the process of calculating uptake within a region.  We introduce a spatial kernel smoothing method in the Bayesian setting that allows for estimation of local vaccine uptake through the combination of administrative and survey data sources.  The newly developed method is applied to pneumococcal conjugate vaccine uptake data from Brazil in 2013.  Results suggest that the method provides estimates of vaccine uptake at local levels that are in closer agreement to collected survey responses than the standard method that ignores the issue of participant mobility.  The method also provides insight into patterns of mobility of vaccine recipients based on the inclusion of region-specific covariates.         

\item \textbf{Harrison Quick}, Drexel University \\
``Spatiotemporal trends in stroke mortality'' \\
Harrison Quick


Geographic patterns in stroke mortality have been studied as far back as the 1960s, when a region of the southeastern United States became known as the "stroke belt" due to its unusually high rates. While stroke mortality rates are known to increase exponentially with age, an investigation of spatiotemporal trends by age group at the county-level is daunting due to the preponderance of small population sizes and/or few stroke events by age group. Our goal here is two-pronged.  First and foremost, we harness the power of a complex, nonseparable multivariate space-time model which borrows strength across space, time, and age group to obtain reliable estimates of yearly county-level mortality rates from US counties between 1973 and 2013 for those aged 65+. Second, we outline how the results of this model fit can be used to generate high-quality synthetic data for public use that preserve data confidentiality without sacrificing data utility.

\end{itemize}

\subsubsection*{8. Statistical Approaches to Data Modeling and Analysis}

\begin{itemize}
\item \textbf{Patrick Flaherty}, University of Massachusetts-Amherst \\
``A Deterministic Global Optimization Method for Variational Inference'' \\
Hachem Saddiki, Andrew C. Trapp, Patrick Flaherty


Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees.
However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum.
Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood.
We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM).
We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound.
We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.

\item \textbf{Matthias Steinruecken}, University of Massachusetts-Amherst \\
``Unraveling the demographic history of modern humans using full-genome sequencing data'' \\
Matthias Steinruecken


Contemporary and ancient demographic structure in human populations has shaped the genomic variation observed in modern humans, and severely affected the distribution of functional and disease related genetic variation. Using next-generation sequencing technologies, researchers gather increasing amounts of genomic sequencing data for large samples in many different human population groups. These datasets present unprecedented opportunities to study genomic variation in complex demographic scenarios, and this area has received a lot of attention in recent years.


In this talk, I will present a method for the inference of demographic histories from full-genome sequencing data of multiple individuals developed by me and my collaborators. I will apply this method to a genomic dataset of Native American individuals to unravel the ancient demographic events underlying the peopling of the Americas. Morevoer, I will discuss a novel method for demographic inference that has the potential to improve inference especially in the recent past, which is of particular importance in the context of complex genetic diseases in humans.

\item \textbf{Evan L. Ray}, University of Massachusetts, Amherst \\
``Feature-Weighted Ensembles for Probabilistic Time-Series Forecasts'' \\
Evan L. Ray, Nicholas G. Reich


Accurate and reliable predictions of infectious disease incidence are important for public health decision makers planning resource allocation and interventions designed to prevent or reduce disease transmission. Ensemble prediction methods, which combine predictions from multiple ``component" models, have recorded superior performance in a variety of tasks from weather prediction to product recommendation; however, applications of ensemble methods in the context of predicting infectious disease have been limited. We considered a range of ensemble methods that each form a predictive density for a target of interest as a weighted sum of the predictive densities from several component models. In the simplest case, equal weight is assigned to each component model; in the most complex case, the weights vary with multiple observed features such as recent observations of disease incidence and the time of the year when predictions are made. We applied these methods to predict measures of influenza season timing and severity in the United States, both at the national and regional levels, using three component models. We trained the models on retrospective predictions from 14 seasons (1997/1998 - 2010/2011) and evaluated each model's prospective, out-of-sample performance in the five subsequent influenza seasons. In this test phase, the ensemble methods showed overall performance that was similar to the best of the component models, but offered more consistent performance across seasons than the component models. Ensemble methods offer the potential to deliver more reliable infectious disease predictions to public health decision makers.

\end{itemize}

\subsubsection*{11. Recent Developments on High-Dimensional Statistics and Regularized Estimation}

\begin{itemize}
\item \textbf{Ethan Fang}, Pennsylvania State University-Main Campus \\
``Blessing of Massive Scale: Spatial Graphical Model  Estimation with a Total Cardinality Constraint Approach'' \\
Ethan Fang, Han Liu, Mengdi Wang


We consider the problem of estimating high dimensional spatial graphical models with a total cardinality constraint. Though this problem is highly nonconvex, we show that its primal-dual gap diminishes linearly with the dimensionality and provide a convex geometry justification of this ``blessing of massive scale" phenomenon. Motivated by this result, we propose an efficient algorithm to solve the dual problem (which is concave) and prove that the solution achieves optimal statistical properties. Extensive numerical results are also provided.

\item \textbf{Cheng Yong Tang}, Temple University \\
``Sufficient dimension reduction with missing data'' \\
Yuexiao Dong, Cheng Yong Tang, Qi Xia


Inverse regressions constitute a class of sufficient dimension reduction methods targeting at estimating the central space by regression-type approaches implemented inversely on the predictors and the responses. The most representative approach in this family is the seminal Sliced Inverse Regression (SIR) approach proposed by Li (1991). In this study, we first show that missing responses generally affect the validity of the inverse regressions under the scheme of the so-called missing at random, in the sense that the resulting estimations for the central space can be biased if data with missing responses are simply ignored.  We then propose two simple and effective adjustments for missing responses that guarantees the validity of the inverse regressions. The proposed methods share the essence and simplicity of the inverse regressions. We demonstrate the performance of the proposed inverse regressions for dealing with missing responses by numerical and theoretical analyses. 

\item \textbf{Ting Zhang}, Boston University \\
``A Thresholding-Based Prewhitened Long-Run Variance Estimator and Its Dependence-Oracle Property'' \\
Ting Zhang


Statistical inference of time series data routinely relies on the estimation of long-run variances, defined as the sum of autocovariances of all orders. The current paper considers a new class of long-run variance estimators, which first soaks up the dependence by a decision-based prewhitening filter, then regularizes autocorrelations of the resulting residual process by thresholding, and finally recolors back to obtain an estimator of the original process. Under mild regularity conditions, we prove that the proposed estimator (i) consistently estimates the long-run variance; (ii) achieves the parametric convergence rate when the underlying process has a sparse dependence structure as in finite-order moving average models; and (iii) enjoys the dependence-oracle property in the sense that it will automatically reduce to the sample variance if the data are actually independent. Monte Carlo simulations are conducted to examine its finite-sample performance and make comparisons with existing estimators.

\end{itemize}

\subsubsection*{12. Subgroup Analysis}

\begin{itemize}
\item \textbf{Wai-Ki Yip}, Foundation Medicine, Inc. \\
``Sr. Biostatistician'' \\
Wai-Ki Yip, Marco Bonetti, Ann Lazar, William Barcella, Victoria Xin Wang, Chip Cole, Rich Gelber


The Subpopulation Treatment Effect Pattern Plot is a visual and statistical technique to explore patterns of treatment effects across values of a continuously measured covariate such as a biomarker measurement. Originally developed specifically for investigation of survival outcomes, it has been extended to continuous, binary and count outcomes.  This talk will focus on the development of this extension – what are the outcomes, the permutation statistics and the Type 1 error, the power of the test, comparison with other methods, and the software.  Then, a motivating example of how it is applied to analyze data from the Aspirin/Folate Polyp Prevention Study will be presented. 
A quick summary of recent research development in STEPP will be presented at the end.

\item \textbf{Yanxun Xu}, Johns Hopkins University \\
``A Nonparametric Bayesian Basket Trial Design'' \\
Yanxun Xu, Peter Mueller, Apostolia Tsimberidou, Donald Berry


Targeted therapies on the basis of genomic aberrations analysis of the tumor have shown promising results in cancer prognosis and treatment. Regardless of tumor type, trials that match patients to targeted therapies for their particular genomic aberrations have become a mainstream direction of therapeutic management of patients with cancer. Therefore, finding the subpopulation of patients who can most benefit from an aberration-specific targeted therapy across multiple cancer types is important. We propose an adaptive Bayesian clinical trial design for patient allocation and subpopulation identification. We start with a decision theoretic approach, including a utility function and a probability model across all
possible subpopulation models. The main features of the proposed design and population finding methods are that we allow for variable sets of covariates to be recorded by different patients, adjust for missing data, allow high order interactions of covariates, and the adaptive allocation of each patient to treatment arms using the posterior predictive probability of which arm is best for each patient. The new method is demonstrated via extensive simulation studies.

\end{itemize}

