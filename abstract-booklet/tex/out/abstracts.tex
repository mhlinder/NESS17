
\subsection*{Morning sessions}

\subsubsection*{1. New Vistas in Statistics with Applications}

\begin{itemize}
\item \textbf{Aleksey Polunchenko}, Binghamton University \\
``Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts Diffusion with Constant Positive Drift'' \\
Aleksey Polunchenko


We consider the first exit time of a Shiryaev-Roberts diffusion with constant positive drift from the interval $[0,A]$ where $A>0$. We show that the moment generating function (Laplace transform) of a suitably standardized version of the first exit time converges to that of the unit-mean exponential distribution as $A\to+\infty$. The proof is explicit in that the moment generating function of the first exit time is first expressed analytically and in a closed form, and then the desired limit as $A\to+\infty$ is evaluated directly. The result is of importance in the area of quickest change-point detection, and its discrete-time counterpart has been previously established - although in a different manner - by Pollak and Tartakovsky (2009).

\item \textbf{Emmanuel Yashchin}, IBM Research \\
``Alarm Prioritization in Early Warning Systems'' \\
Emmanuel Yashchin


In complex manufacturing and business operations, early warning systems (EWSs) ensure timely detection of unfavorable trends. Such systems can be deployed so that they act as search engines, analyzing available data at time points that are either pre-specified or determined based on process information. A round of analysis typically encompasses a large number of data streams that are governed by an even larger set of statistical parameters. Careful design of monitoring procedures ensures a low rate of false alarms. To ensure efficient utilization of personnel, it is important that these alarms are properly prioritized. We discuss methods and statistics relevant in the process of alarm prioritization, and their use in the field of integrated circuit manufacturing. 

\item \textbf{Zuofeng Shang}, Binghamton University \\
``Computationally Efficient Nonparametric Testing'' \\
Zuofeng Shang, Meimei Liu, Guang Cheng


A recent trend of big data problems is to develop computationally efficient inferential methods that embed computational thinking into uncertainty quantification. In this talk I will introduce two new classes of nonparametric testing that scale well with large datasets. One class is based on randomized sketches which can be implemented in one computer, while the other class requires parallel computing. Our theoretical contribution is to characterize the minimal computational cost that is needed to achieve testing optimality. Optimal estimation is a byproduct. The proposed methods are examined by simulated and real datasets.

\item \textbf{Vasanthan Raghavan}, Qualcomm \\
``Non-Parametric Approaches to Change Detection'' \\
Vasanthan Raghavan


In this talk, we pursue a statistical non-parametric approach for spurt and downfall detection in activity profiles corresponding to real-time applications. While parametric approaches involve learning the underlying parameters, our approach is based on binning the count data of activity to form observation vectors that can be compared with each other. Motivated by a majorization theory framework, these vectors are then transformed via certain functionals and used in spurt classification. While the parametric approaches often result in either a large number of missed detections of real changes or false alarms, the proposed approach is shown to result in a small number of missed detections and false alarms. Further, the non-parametric nature of the approach 
makes it attractive for ready applications in a practical context. Performance of the proposed approach in a real terrorist monitoring application are presented. 

\end{itemize}

\subsubsection*{3. Space-Time Statistical Solutions at IBM Research}

\begin{itemize}
\item \textbf{Julie Novak}, IBM Research \\
``Statistical Challenges of Large-Scale Revenue Forecasting'' \\
Julie Novak, Stefa Etchegaray Garcia, Yasuo Amemiya


Large-scale businesses need to have a clear vision of how well they expect to perform within all their different units. This information will directly impact managerial decisions that will in turn affect the future health of the company. In this talk, we focus on the statistical challenges that occur when implementing our revenue forecasting methodology on a weekly basis within a large business. We must provide reasonably accurate forecasts for all the geography/division combinations, which have fundamentally different revenue trends and patterns over time. Our method must be robust to “oddities”, such as typos in the input or unusual behavior in the data. In addition, our forecasts must be stable over weeks, without sacrificing on accuracy. We describe the statistical methods used to maintain an efficient and effective operational solution. 

\item \textbf{Yasuo Amemiya}, IBM T. J. Watson Research Center \\
``Spatio-Temporal Analysis for System Management'' \\
Yasuo Amemiya, Youngdeok Hwang


IBM has been providing analytics-based solutions to various large-scale problems relevant for business, government, and society.  A goal of such a project is to manage a large physical system effectively based on analysis of various measurements taken over space and time.  Statistical analysis methods and ideas are essential part of the overall solution development.  In particular, new types of spatio-temporal analysis methods are needed.  In this talk, some of large system management projects at IBM Research are described, and the development of appropriate spatio-temporal analysis methods is discussed.

\item \textbf{Xiao Liu}, IBM Thomas J. Watson Research Center \\
``a Spatio-Temporal Modeling Framework for Weather Radar Image Data in Tropical Southeast Asia'' \\
Xiao Liu,   Vik Gopal,   Jayant Kalagnanam


Tropical storms are known to be highly chaotic and extremely difficult to predict. In tropical countries such as Singapore, the official lead time for the warnings of heavy storms is usually between 15 and 45 minutes because weather systems develop quickly and are of very short lifespan. A single thunderstorm cell, for example, typically lives for less than an hour. Weather radar echoes, correlated in both space and time, provide a rich source of information for short-term precipitation nowcasting. Based on a large dataset of 276 tropical storms events, this work investigates a spatio-temporal modeling approach for two-dimensional radar reflectivity (echo) fields. Under a Lagrangian integration scheme, we model the radar reflectivity field by a spatio-temporal conditional autoregressive process with two components. The first component is the dynamic velocity field which determines the motion of the storm, and the second component governs the growth or decay of the returned radar echoes. The proposed method is demonstrated and compared with existing methods using real radar image data collected from a number of 276 tropical storm events from 2010 to 2011 in Singapore. The advantage of the proposed method is found in modeling small-scale localized convective weather systems, which are the most important type of storm during the Inter-Monsoon Season in Southeast Asia.

\item \textbf{Rodrigue Ngueyep}, IBM Thomas J. Watson Research Center \\
``Spatial Segmentation of Spatial-Temporal Lattice Models for Agricultural Management Zoning'' \\
Rodrigue Ngueyep, Huijing Jiang, Youngdeok Hwang


In many applications where both predictors and responses are collected across geographical regions over time, the impact of the predictors to responses are often not static but time-varying. Moreover, the time-varying impact of the predictors may vary across different regions. To identify nearby regions where these time-varying impact behave similarly, we propose a spatially fused time-varying lattice model. We model time-varying impact of spatio-temporal predictors via a spatial lattice model with time-varying coefficients. Furthermore, we utilize fusion penalty to allow nearby regions to share same time-varying coefficients. The model parameters can be efficiently estimated via ADMM algorithm. One motivation application of our method is to identify agriculture management zones where the time-varying impact of environment attributes (e.g., growing degree days, heat stress,precipitation) on the crop yield is similar. Once these zones are identified, same planting policy could be implemented within these zones.

\end{itemize}

\subsubsection*{4. Graphical Models, Networks, Regulatome and Multivariate Analysis}

\begin{itemize}
\item \textbf{Kuang-Yao Lee}, Yale University \\
``Learning Causal Networks via Additive Faithfulness'' \\
Kuang-Yao Lee, Tianqi Liu, Bing Li, and Hongyu Zhao


In this work, we propose a statistical model called additively faithful directed acyclic graph (AFDAG) for causal learning from observational data. Our approach is based on additive conditional independence (ACI), a recently proposed three-way statistical relation that shares many similarities with conditional independence but without resorting to multivariate kernels. This distinct feature strikes a balance between a parametric model and a fully nonparametric model, which makes the proposed model attractive to large networks. For graph inference, we develop an estimator  for AFDAG  based on a linear operator  that characterizes ACI, and establish the consistency and convergence rates of our estimator. Moreover, we prove the uniform consistency of the estimated DAG under a stronger additive faithfulness condition, which appears to be less restrictive than its linear counterpart. We introduce a modified PC-algorithm to implement the estimating procedures efficiently, so that their complexity is determined by the level of sparseness rather than the dimension of the network. Through simulation studies we show that our method outperforms  existing methods when commonly assumed conditions such as Gaussian or Gaussian copula distributions do not hold. Finally, the usefulness of AFDAG formulation is demonstrated through an application to a proteomics data set

\end{itemize}

\subsubsection*{5. Big Data}

\begin{itemize}
\item \textbf{Li Ma}, Duke University \\
``Fisher Exact Scanning for Dependency'' \\
Li Ma, Jialiang Mao


We introduce a method—called Fisher exact scanning (FES)—for testing and identifying variable dependency that generalizes Fisher’s exact test on 2-by-2 contingency tables to R-by-C contingency tables and continuous sample spaces. FES proceeds through scanning over the sample space using windows in the form of 2-by-2 tables of various sizes, and on each window completing a Fisher’s exact test. Based on a factorization of Fisher’s multivariate hypergeometric (MHG) likelihood into the product of the univariate hypergeometric likelihoods, we show that there exists a coarse-to-fine, sequential generative representation for the MHG model in the form of a Bayesian network, which in turn implies the mutual independence (up to deviation due to discreteness) among the Fisher’s exact tests completed under FES. This allows an exact characterization of the joint null distribution of the p-values and gives rise to an effective inference recipe through simple multiple testing procedures such as Sidak and Bonferroni corrections, eliminating the need for resampling. In addition, FES can characterize dependency through reporting significant windows after multiple testing control. The computational complexity of FES scales linearly with the sample size, which along with the avoidance of resampling makes it ideal for analyzing massive data sets. We use extensive numerical studies to illustrate the work of FES and compare it to several state-of-the-art methods for testing dependency in both statistical and computational performance. Finally, we apply FES to analyzing a microbiome data set and further investigate its relationship with other popular dependency metrics in that context.

\item \textbf{Yuwen Gu}, University of Minnesota \\
``Penalized Composite Quantile Regression for High-Dimensional Data'' \\
Yuwen Gu


Composite quantile regression (CQR) provides efficient estimation of the coefficients in linear models, regardless of the error distributions. We consider penalized CQR for both variable selection and efficient coefficient estimation in a linear model under ultrahigh dimensionality and possibly heavy-tailed error distribution. Both lasso and folded concave penalties are discussed. An L2 risk bound is derived for the lasso estimator to establish its estimation consistency and strong oracle property of the folded concave penalized CQR is shown for a feasible solution via the LLA algorithm. The nonsmooth nature of the penalized CQR poses great numerical challenges for high-dimensional data. We provide a unified and effective numerical optimization algorithm for computing penalized CQR via ADMM. We demonstrate the superior efficiency of penalized CQR estimator, as compared to the penalized least squares estimator, through simulated data under various error distributions. 

\item \textbf{Jacob Bien}, Cornell University \\
``Learning Local Dependence in Ordered Data'' \\
Guo Yu, Jacob Bien


In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex optimization problem that decomposes into independent subproblems that can be solved efficiently in parallel. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure.  Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium.

\end{itemize}

\subsubsection*{6. Bayesian Applications in High-Dimensional and Multivariate Modeling}

\begin{itemize}
\item \textbf{Seongho Song}, University of Cincinnati \\
``Bayesian Multivariate Gamma-Frailty Cox Model for Clustered Current Status Data”'' \\
Negar Jaberansari, Dipak K. Dey and Seongho Song


Biomedical data analysis plays a key role in today's medicine. Multivariate current status data is a common type of Biomedical data which gives rise to two main challenges in data analysis. First, all event times are censored, making censoring times the only indicator of event occurrence. Second, an unobserved heterogeneity caused by clusters of units or individuals is probable. To address these issues, mixed Cox proportional hazard model with random block frailty has been used. Here, we consider a Bayesian multivariate Gamma-frailty Cox model and augment the likelihood with respect to random frailties and a set of Poisson latent variables. We also introduce a novel MCMC algorithm by employing two diff erent cumulative baseline hazard function structures: a transformed mixture of incomplete Beta distributions and a linear combination of monotone integrated splines. Through several simulations, we show that our methodology achieves competitive results. We also compare the performance of the two baseline hazard functions using model selection criteria such as AIC and DIC. Finally, we apply the model to a bivariate current status cataract dataset and investigate the e ffect of various risk factors on the occurrence of
cataracts.

\item \textbf{Gyuhyeong Goh}, Kansas State University \\
``Bayesian Variable Selection using Marginal Posterior Consistency'' \\
Gyuhyeong Goh, Dipak K. Dey


Due to recent technological advancements, high-dimensional data are frequently involved in many areas of science. When an extreme large number of possible predictors are under consideration for the data, marginal likelihood estimation provides an effective way to reduce the high-dimensionality. However, the marginal likelihood-based approach ignores simultaneous influence of predictors and often leads to misidentification of relevant predictors. In this paper, we propose a new variable selection procedure for accounting for the joint influence of important predictors. We use marginal posterior distributions to incorporate all possible predictor effects into the variable selection procedure. Some theoretical properties of the proposed method are investigated. A simulation study demonstrates that our Bayesian approach provides better variable selection performance than existing marginal likelihood methods.

\end{itemize}

\subsubsection*{7. New Advances in Analysis of Complex Data: Heterogeneity and High Dimensions}

\begin{itemize}
\item \textbf{Dan Yang}, Rutgers University-New Brunswick \\
``Bilinear Regression with Matrix Covariates in High Dimensions'' \\
Dong Wang, Hongtu Zhu, and Haipeng Shen


Traditional functional linear regression usually takes a one dimensional functional predictor as input and estimates the continuous coefficient function. Modern applications often generate two dimensional covariates, which when observed at grid points are matrices. To avoid inefficiency of the classical method involving estimation of a two dimensional coefficient function, we propose a bilinear regression model and obtain estimates via a smoothness regularization method. The proposed estimator exhibits minimax optimal property for prediction under the framework of Reproducing Kernel Hilbert Space. The merits of the method are further demonstrated by numerical experiments and an application on real imaging data. 

\item \textbf{Yiyuan She}, Florida State University \\
``On Cross-Validation for Sparse Reduced Rank Regression'' \\
Yiyuan She, Hoang Tran


In high-dimensional data analysis, regularization methods pursuing sparsity and/or low rank have received a lot of attention recently.  To provide a proper amount of shrinkage, it is typical to use a grid search and a model comparison criterion to find optimal regularization parameters. However, we show that fixing the parameters   across all folds may result in an inconsistency issue, and  it is more appropriate to cross-validate projection-selection patterns to obtain the best coefficient estimate. Our     in-sample error studies in jointly sparse and rank-deficient models lead to a new class of information criteria with four scale-free forms to bypass the estimation of  noise level. By use of an identity, we propose a novel scale-free  calibration to help cross-validation achieve the minimax optimal error rate non-asymptotically. Extensive simulations support the efficacy of the proposed methods.

\end{itemize}

\subsubsection*{8. Machine Learning and Big Data Analytics}

\begin{itemize}
\item \textbf{Renato Polimanti}, Yale University \\
``Resources to Investigate the Genetic Architecture of Complex Traits: Large-Scale  Datasets and Summary Association Data'' \\
Renato Polimanti


Complex traits include a wide range of phenotypes from common diseases to physiological characteristics and their predisposition is generally related to the contribution of hundreds to thousands of variants with small effect. Genome-wide association studies (GWAS) are a powerful approach to detect these small-effect loci, providing an unbiased information to dissect the key biological mechanisms of complex traits. However, to be informative, GWAS require massive cohorts and many genomic consortia, including hundred investigators, are continuing to pool together samples to increase GWAS sample size and consequently their statistical power. There is a pervasive genetic correlation among complex traits due to causal relationships and shared molecular mechanisms. Thus, GWAS results can be used to conduct follow-up investigations, such as polygenic risk score analysis, Mendelian randomization, and phenome-wide association studies, to dissect the genetic architecture of additional traits. Different analytic approaches can be used to conduct these follow-up investigations using large datasets with a wide range of phenotypic information and summary association data. In this talk, I will give an overview of GWAS and post-GWAS studies, describe the current challenges and limitations, and discuss the motivations to develop new statistical methods to investigate the genetics of complex traits.

\item \textbf{Michael Kane}, Yale University \\
``a First Look at using Human Mobility Data to Assess Community Resilience'' \\
Michael Kane


We use cell tower data to measure communities' ability to respond, withstand and recover from adverse situations. There have recently been calls for significant investments in infrastructure to develop and fortify community resilience. However, resilience has been a difficult concept to quantify, and associated studies tend to be ad-hoc or community specific. We will explore the use of aggregate movement of individuals within a community to measure resiliency. The effect of Hurricane Matthew is used as a case study. We will show the effect of the hurricane on human mobility and identify communities that were particularly hard-hit and slow to respond.

\item \textbf{Sheida Nabavi}, University of Connecticut \\
``Statistical Machine Learning to Identify Candidate Drivers of Drug Resistance in Cancer'' \\
Sheida Nabavi


With advances in technologies, huge amounts of multiple types of high-throughput genomics data are available. These data have tremendous potential to identify new and clinically valuable biomarkers to guide the diagnosis, assessment of prognosis, and treatment of complex diseases. Integrating, analyzing, and interpreting big and noisy genomics data to obtain biologically meaningful results, however, remains highly challenging. Utilizing statistical machine learning methods can help to address these issues.
To facilitate the identification of a short list of biologically meaningful genes as candidate drivers of anti-cancer drug resistance from an enormous amount of heterogeneous data, we employed statistical machine-learning techniques and integrated genomics datasets. We developed a computational method that integrates gene expression, somatic mutation, and copy number aberration data of sensitive and resistant tumors. In this method, an integrative method based on regression tree and module network analysis is applied to identify potential driver genes. We applied this method to the ovarian cancer data from the cancer genome atlas. The method yields a short list of aberrant genes that also control the expression of their co-regulated genes. The final result contains biologically relevant genes, such as COL11A1, which has been recently reported as a cis-platinum resistant biomarker for ovarian carcinoma. 

\end{itemize}

\subsubsection*{9. Statistical Approaches in Modeling and Incorporating Dependence}

\begin{itemize}
\item \textbf{Mengyu Xu}, University of Central Florida \\
``Pearson's Chi-Squared Statistics: Approximation Theory and Beyond'' \\
Mengyu Xu, Danna Zhang, Wei Biao Wu


We establish a Chi-squared approximation theory for Pearson's Chi-squared statistics by using a high-
dimensional central limit theorem for quadratic forms of random vectors. Our high-
dimensional central limit theorem or invariance principle is proved under Lyapunov-
type conditions that involve a delicate interplay between the dimension p, the sample
size n and the moment condition. To obtain cutoff values of our tests, we introduce a plug-in Gaussian multiplier calibration method and normalized consistency, a new matrix convergence criterion. Based on our modified Chi-squared statistic, we propose the concept of adjusted degrees of freedom. We develop a Cramer-von Mises type test for testing distributions of high dimensional data and develop an approximation theory based on our invariance principle.

\item \textbf{Kun Chen}, University of Connecticut \\
``Regularized Mixture Regression with Mixed and Incomplete Outcomes'' \\
Jian Liang, Kun Chen, Fei Wang


In many real-world problems, the collected outcomes are of mixed types, including continuous measurements, binary indicators and counts, and the data may also subject to substantial missing values. Regardless of their types, these mixed outcomes are often interrelated, representing diverse reflections or views of the same underlying data generation mechanism. In particular, these correlated outcomes could be linked through sharing the same population heterogeneity. We develop a regularized finite mixture model that 1) finds sample clusters and jointly models incomplete mixed-type responses, 2) achieves shared predictor selection between responses and cluster components, and 3) detects and accommodates outliers. We provide oracle performance bounds for our model under a high-dimensional non-asymptotic framework. Owing to its integrative learning scheme, our model is particularly effective for the imputation of incomplete outcomes. Simulations and data examples show that our method achieves state-of-the-art performance.

\item \textbf{Liliya Lavitas}, Boston University \\
``Unsupervised Self-Normalized Change-Point Testing for Time Series'' \\
Ting Zhang, Liliya Lavitas


We propose a new self-normalized method for testing change points in the time series setting. Self-normalization has been celebrated for its ability to avoid direct estimation of the nuisance asymptotic variance and its flexibility of being generalized to handle quantities other than the mean. However, it was developed and mainly studied for constructing confidence intervals for quantities associated with a stationary time series, and its adaptation to change-point testing can be nontrivial as direct implementation can lead to tests with nonmonotonic power. Compared with existing results on using self-normalization in this direction, the current paper proposes a new self-normalized change-point test that does not require prespecifying the number of total change points and is thus unsupervised. In addition, we propose a new contrast-based approach in generalizing self-normalized statistics to handle quantities other than the mean, which is specifically tailored for change-point testing. Monte Carlo simulations are presented to illustrate the finite-sample performance of the proposed method.

\item \textbf{Buddika Peiris}, Worcester Polytechnic Institute \\
``Assistant Teaching Professor.'' \\
Buddika Peiris.


Regression analysis constitutes a large portion of the statistical repertoire in applications. In case
where such analysis is used for exploratory purposes with no previous knowledge of the structure
one would not wish to impose any constraints on the problem. But in many applications we are
interested in a simple parametric model to describe the structure of a system with some prior
knowledge of the structure. An important example of this occurs when the experimenter has the
strong belief that the regression function changes monotonically in some or all of the predictor
variables in a region of interest. Especially in autoregressive models, one can assume that some of
parameters are positive (or negative). The analyses needed for statistical inference under such
constraints are nonstandard. The specific aim of this study is to introduce a technique, which can
be used for statistical inferences of a multiple linear regression with some non-standard constraints.

\item \textbf{Forrest W. Crawford}, Yale University \\
``Causal Inference for Network Epidemics'' \\
Forrest W. Crawford, Olga Morozova, Xiaoxuan Cai, Ted Cohen


Estimating the effect of an infectious disease intervention
in a single interconnected population is challenging because subjects
may transmit infection to others. Preventive treatment (e.g.
vaccination) may exert a direct effect on the person who receives it,
and may protect others by preventing infection in the recipient and
thereby transmission to someone else, or by reducing the recipient’s
infectiousness when infected. We outline a causal framework for
estimating the direct and indirect effects of a vaccine in a single
networked population. Epidemiological assumptions collapse subjects’
outcomes over time into cumulative exposures experienced by
susceptible subjects; conditional independence assumptions permit
identification of the direct effect and partition of the indirect
effect into distinct effects on susceptibility and transmissibility. I
describe a semi-parametric class of infectious disease regression
models motivated by a continuous-time Markov stochastic epidemic
process. I describe the consequences of misspecification of the
infection model, and discuss approaches to estimation when either the
network or time series of infections are incompletely observed.

\end{itemize}

\subsubsection*{10. Biopharmaceutical Statistics}

\begin{itemize}
\item \textbf{Joseph C. Cappelleri}, Pfizer Inc \\
``Meta-Analysis of Safety Data in Clinical Trials'' \\
Joseph C Cappelleri


Meta-analyses of clinical trial safety data have risen in importance beyond regulatory submissions. During drug development, pharmaceutical sponsors need to recognize safety signals early and adjust the development program accordingly, so as to facilitate the assessment of causality. Once a medicinal product is marketed, sponsors add post-approval clinical trial data to the body of information to help understand existing safety concerns or those that arise from other post-approval data sources, such as spontaneous reports. The situation becomes more involved when interest centers on a network comparison of multiple active treatments.  This presentation highlights some of the major issues considered in meta-analysis of safety data – such as sparse events, reporting quality, and limited study duration – and identifies gaps requiring special attention.   

\item \textbf{Qiqi Deng}, Boehringer Ingelheim \\
``Choosing Timing and Boundary for Futility Analysis Based on Cost-Effective Assessment'' \\
QiQi Deng, Xiaoqi Lu


When a futility analyses is included in a trial, it’s important to choose the right timing for the interim analysis as well as an appropriate futility boundary, so that the trial is likely to be stopped when the interim data suggests a reasonable treatment effect dose not likely exist. This idea is appealing from an ethical point of view since it may reduce the exposure of patients to ineffective treatments, and from a financial point of view since phase III trials are usually the most significant investment in drug development. However, the design may become inefficient if timing and boundary of futility analysis are not chosen carefully. In this presentation, we will use cost-effectiveness analysis to assess the performance of different futility rules, and introduce a graphical tool to guide the selection of design parameters. In addition, we will discuss how prior information/belief of the treatment effect and other factors may influence the futility decision.

\item \textbf{Abidemi Adeniji}, EMD Serono \\
``Estimation of Discrete Survival Function Through the Modeling of Diagnostic Accuracy for Mismeasured Outcome Data'' \\
Hee-Koung Joeng, Abidemi K. Adeniji, Naitee Ting and Ming-Hui Chen


Standard survival methods are inappropriate for mismeasured outcomes. Previous research has shown that outcome misclassification can bias estimation of the survival function. We develop methods to accurately estimate the survival function when the diagnostic tool used to measure the outcome of disease is not perfectly sensitive and specific. Since the diagnostic tool used to measure disease outcome is not the gold standard, the true or error-free outcomes are latent, they cannot be observed. Our method uses the negative predictive value (NPV) and the positive predictive values (PPV) of the diagnostic tool to construct a bridge between the error-prone outcomes and the true
outcomes. We formulate an exact relationship between the true (latent) survival function and the observed (error-prone) survival function as a formulation of time-varying NPV and PPV. We specify models for the NPV and PPV that depend only on parameters that can be easily estimated from a fraction of the observed data. Furthermore, we conduct an in depth study to accurately estimate the latent survival function based on the assumption that the biology that underlies the disease process follows a stochastic process. We further examine the performance of our method by applying it to the VIRAHEP-C data.

\item \textbf{Bushi Wang}, Boehringer Ingelheim \\
``How to Evaluate Type Ii Error Rate with Multiple Endpoints'' \\
Bushi Wang; Naitee Ting


The FDA draft guidance on multiple endpoints in clinical trials (January 2017) pointed out the regulatory concern of the type II error rate inflation with multiple endpoints. Many of the statistical adjustment to control the type I error rate for multiplicity decrease the study power because they lowered the alpha level used for each of the individual endpoint. The use of co-primary endpoints does not require multiplicity adjustment for type I error but will also increase the type II error rate and decrease study power. In this presentation, I provide a few detailed steps on how to evaluate sample size based on the objective of the clinical study and the selected multiplicity adjustment to control type I error. Analytic forms of power for individual endpoint hypothesis can be derived for most commonly seen scenarios. Simulation can be also easily set up. Optimal sample size is possible by fine tune the individual power for each endpoint with different effect size assumptions.

\item \textbf{Donald Bennett}, Pfizer \\
``Nonclinical Statistics in Drug Development: In Vitro and in Vivo Examples'' \\
Donald Bennett


An introductory overview of nonclinical statistics critical role in the drug development process will be reinforced by real world examples.  Discussion of issues in non-linear models for in vitro assays combined with examples of in vivo experimental designs and analysis will be used to highlight the impact statisticians make in early drug development.  We will discuss statistical methods and training needed for successful statistical contribution in the preclinical space.  We will also review the expectations of preclinical scientific collaborations and best practices in contrast with the role of clinical statisticians in drug development.

\item \textbf{Jerry Lewis}, Biogen \\
``Outlook on Outliers'' \\
Jerry Lewis


Outlier testing has a long and checkered history in the pharmaceutical/biopharmaceutical industry, and has undergone close regulatory scrutiny since the Barr decision (U.S.A. vs. Barr Labs, 1993).  This talk will include some basic observations on the philosophy and practice of outlier testing and proceed to the development of a novel outlier test for plate based relative potency dilution bioassays with limited replication.

\item \textbf{Ray Liu}, Takeda \\
``Big Data, Statistical Innovation and Impact on Drug Development'' \\
Ray Liu, Cong Li, Jacob Zhang


Drug development is a lengthy process. Speeding up the drug development is a win-win for both patients and pharma companies. Thanks to the advancing technologies that generate data of new modalities with much richer information, the ‘Big Data’ has seen its adaption into drug development in the past 3-5 years. While ‘Big Data’ holds the promise to make drug development more efficient, its characteristics of high ‘volume’, ‘velocity’ and ‘variety’ present unique challenges to existing analytical infrastructures. In this talk I will use real examples to demonstrate how we use statistical innovations within Takeda to maximize the potential of Big Data and shorten the drug development.

\item \textbf{Chi-Hse Teng}, Novartis \\
``Finding Needles in a Hay Stack – an Approach for a Small-Number-Factor High-Dimensional Data'' \\
Chi-Hse Teng and Azita Ghodssi


We present a data analysis approach for a project that engineered mRNA constructs to max expression and half-life of a secreted protein.  We selected 22 3’UTRs, 105 5’UTRs and 93 Signal peptides of highly expressed, secreted skeletal muscle proteins to test their relative influences on expression of the protein of interest.  Given 22 3’UTRs, 105 5’UTRs and 93 Signal peptides, an all-encompassing experiment would have required 214,830 constructs to include all combinations. This would have been an overwhelming effort. The screen was limited to an examination of 218 constructs instead.  The data analysis effort is to identify 3’UTRs, 5’UTRs and Signal peptides that might increase the expression of protein.   We developed an approach to rank the sequences of each region.  The approach delivered sensible results matching the experimental data of some known sequence’s ranking.   It also performed well in the simulation cases.

\end{itemize}

\subsubsection*{11. Extremes}

\begin{itemize}
\item \textbf{John Nolan}, American University \\
``Mvevd: An r Package for Extreme Value Distributions'' \\
Anne-Laure Fougeres, Cecile Mercadier, John Nolan


We present a new way to estimate multivariate extreme value distributions (MVEVD) from data using max projections.
The approach works in any dimension, though computation time increases quickly as dimension increases.
The procedure requires tools from computational geometry and multivariate integration techniques.
An R package mevd is being developed to implement the method for several semi-parametric classes of MEVDs:
discrete angular measure, generalized logistic, piecewise linear angular measures, and Dirichlet mixture models.

\item \textbf{Karthyek Murthy}, Columbia University in the City of New York \\
``Distributionally Robust Extreme Value Analysis'' \\
Jose Blanchet, Karthyek Murthy


Typical studies in distributional robustness involve computing worst-case bounds for the quantity of interest (such as expected risk, probability of default, etc.) regardless of the probability distribution used, as long as the distribution lies within a prescribed tolerance (measured in terms of a probabilistic divergence like KL divergence) from a suitable baseline model. 

With this practice of computing worst-case bounds over probabilistic distance based neighborhoods gaining popularity, we go beyond the standard choice of KL divergence to study the role of putative model uncertainty in the context of estimation of tail probabilities or quantiles. In particular, we precisely   characterise  the worst-case extreme value index in order to answer “how heavy the tails of neighboring distributions can be?”. This study seeks to understand the qualitative properties of probabilistic distance based neighborhoods in order to guide the selection of model ambiguity regions for estimating extreme quantiles.  

\item \textbf{Tiandong Wang}, Cornell University \\
``Asymptotic Normality of in- And Out-Degree Counts in a Preferential Attachment Model'' \\
Tiandong Wang, Sidney Resnick


Preferential attachment in a directed scale-free graph is an often used paradigm for modeling the evolution of social networks. Social network data is usually given in a format allowing recovery of the number of nodes with in-degree i and out-degree j. Assuming a model with preferential attachment, formal statistical procedures for estimation can be based on such data summaries. Anticipating the statistical need for such node-based methods, we prove asymptotic normality of the node counts. Our approach is based on a martingale construction and a martingale central limit theorem.

\item \textbf{Jingjing Zou}, Columbia University in the City of New York \\
``Extreme Value Analysis without the Largest Values: What can be Done?'' \\
Jingjing Zou, Richard A. Davis, Gennady Samorodnitsky


In this paper we are concerned with the analysis of heavy-tailed data when a portion of the extreme values are unavailable. 
This research was motivated by an analysis of the degree distributions in a large social network. The degree distributions of such networks tend to have power law behavior in the tails. 
We focus on the Hill estimator, which plays a starring role in heavy-tailed modeling. 
The Hill estimator for this data exhibited a smooth and increasing ``sample path'' as a function of the number of upper order statistics used in constructing the estimator.  
This behavior became more apparent as we artificially removed more of the upper order statistics. Building on this observation, we introduce a new parameterization into the Hill estimator that is a function of $\delta$ and $\theta$, that correspond, respectively, to the proportion of extreme values that are unavailable and the proportion of upper order statistics used in the estimation.
As a function of $(\delta, \theta)$, we establish functional convergence of the normalized Hill estimator to a Gaussian random field. An estimation procedure is developed based on the limit theory to estimate the number of missing extremes and extreme value parameters including the tail index and the bias of Hill's estimate. We illustrate how this approach works in both simulations and real data examples.

\end{itemize}

\subsubsection*{12. Feinberg Memorial Session: Bayesian Statistics with Applications}

\begin{itemize}
\item \textbf{Dilli Bhatta}, University of South Carolina Upstate \\
``a Bayesian Test of Independence in a Two-Way  Contingency Table  under Two-Stage Cluster Sampling with Covariates'' \\
DIlli Bhatta, Balgobin Nandram, Joseph Sedransk


We consider a Bayesian approach for the test of independence to study the association between two categorical variables with covariates using data from a two-stage cluster sampling design. Under this approach, we convert the cluster sample with covariates into an equivalent simple random sample without covariates which provides a surrogate of the original sample. Then, this surrogate sample is used to compute the Bayes factor to make an inference about independence.
We apply our methodology to the data from the Trend in International Mathematics and Science Study (2007) for fourth grade U.S. students to assess the association between the mathematics and science scores represented as categorical variables. 
We show that if there is strong association between two categorical variables, there is no significant difference between the tests with and without the covariates. We also performed a simulation study to further understand the effect of covariates in various situations. We found that in borderline cases (moderate association between the two categorical variables) there are noticeable differences in the test with and without covariates. 

\end{itemize}

\subsection*{Afternoon sessions}

\subsubsection*{2. Statistical Applications in Finance and Insurance}

\begin{itemize}
\item \textbf{Liang Peng}, Georgia State University \\
``Professor'' \\
Liang Peng


Testing for predictability of asset returns has been a long history in economics and finance. Recently, based on a simple predictive regression, Kostakis, Magdalinos and Stamatogiannis (2015,  Review of Financial Studies) derived a Wald type test based on the context of the extended instrumental variable (IVX) methodology for testing predictability of stock returns and Demetrescu (2014) showed that the local power of the standard IVX-based test could be improved in some cases when a lagged predicted variable is added to the predictive regression on purpose, which poses a general important question on whether a lagged predicted variable should be included in the model or not. This paper proposes novel robust   procedures for testing both the existence of a lagged predicted variable and the predictability of asset returns in a predictive regression regardless of regressors being stationary or nearly integrated or unit root. A simulation study confirms the good finite sample performance of the proposed tests. We further apply the proposed tests to some real datasets in finance to illustrate their usefulness in practice.

\item \textbf{Brien Aronov, Aritra Halder, Matthew Lamoureux and Shariq Mohammed}, University of Connecticut and Travelers Insurance \\
``Modelling of Large Insurance Claims and Occurrence Data: a Uconn - Travelers Partnership'' \\
Brien Aronov, Kun Chen, Dipak Dey, Aritra Halder, Siddhesh Kulkarni, Matthew Lamoureux, Shariq Mohammed, Elizabeth Schifano and Xiaojing Wang


This joint presentation features the partnership between Travelers Insurance and the Department of Statistics, University of Connecticut, on analyzing big auto insurance claim data to improve spatial risk classification. In our first student project, we explore a spatial variant of the double generalized linear model (DGLM), in which Tweedie distribution, as a special case, is used to model the pure premium, and the spatial correlation is incorporated via Laplacian regularization. The estimated spatial effects are then used to generate risk rankings at the county level. Simulation results and real data analysis showcase the efficacy of the new methods. Besides our recent progress, the challenges we face in large-scale predictive modeling and our future directions will also be discussed. In particular, we focus on collision data and build models for each state separately.

\item \textbf{Fangfang Wang}, Uconn \\
``a Common Factor Analysis of Stock Market Trading Activity'' \\
Zhuowang Li, F. Wang


In this talk, we will study the intraday trading activity of U.S. stocks from ten sectors using a new class of parameter-driven models for multivariate count time series that may not be stationary. With the  model proposed by F. Wang and H. Wang (2016), we formulate the mean process of trading volume and frequency as the product of modulating factors and unobserved stationary processes. The former characterizes the long-run movement in the trading activity, while the latter is responsible for rapid fluctuations and other unknown or unavailable exogenous/endogenous covariates. The unobserved stationary processes evolve independently of the past observed counts, and might interact with each other. The unobserved processes are further modeled as a linear combination of possibly low-dimensional common factors that govern the contemporaneous and serial correlation within and across the observed counts. With the proposed models, we extract the common factors from the intraday trading volume and  frequency of 40 liquid stocks in the first quarter of 2012. Dynamic relationship between common factors adjusted by their associated loading and intraday volatility also investigated.   

\item \textbf{Oleksii Mostovyi}, Uconn \\
``Sensitivity Analysis of the Expected Utility Maximization Problem with Respect to Model Perturbations'' \\
Oleksii Mostovyi, Mihai Sirbu


We study the sensitivity of the expected utility maximization problem in a continuous semimartingale market with respect to small changes in the market price of risk. Assuming that the preferences of a rational economic agent are modeled with a general utility function, we obtain a second-order expansion of the value function, a first-order approximation of the terminal wealth, and construct trading strategies that match the indirect utility function up to the second order. If a risk-tolerance wealth process exists, using it as a numeraire and under an appropriate change of measure, we reduce the approximation problem to a Kunita-Watanabe decomposition. This talk is based on the joint work with Mihai Sirbu. 

\end{itemize}

\subsubsection*{3. Application of Statistical/Predictive Modeling in Health Related Industry}

\begin{itemize}
\item \textbf{Zhaonan Sun}, IBM Research \\
``Exploiting Convolutional Neural Network for Risk Prediction with Medical Feature Embedding'' \\
Zhengping Che, Yu Cheng, Zhaonan Sun, Yan Liu


The widespread availability of electronic health records (EHRs) promises to usher in the era of personalized medicine. However, the problem of extracting useful clinical representations from longitudinal EHR data remains challenging, owing to the heterogeneous, longitudinally irregular, noisy and incomplete nature of such data.

In this talk, we will focus on the problems of high dimensionality and temporality. We explore deep neural network models with learned medical feature embedding to deal with these issues. Specifically, we use a multi-layer convolutional neural network (CNN) to parameterize the model and is thus able to capture complex non-linear longitudinal evolution of EHRs. To account for high dimensionality, we extended the word2vec model and use the embedded medical features in the CNN model. Experiments on real-world EHR data demonstrate the effectivenes of the proposed method. 

\item \textbf{Xiaoyu Jia}, Icahn School of Medicine at Mount Sinai \\
``Opportunities and Challenges in Leveraging Results from Analysis of National Cancer Data Base (Ncdb): a Call for Improvement in Quality and Reproducibility'' \\
Xiaoyu Jia, Madhu Mazumdar


Use of national registry databases for performing comparative effectiveness research is on rise as they present wonderful opportunity for answering questions about the effectiveness of treatments in the adjuvant or neoadjuvant setting and the associations of patient or tumor characteristics with treatment selection and clinical outcomes.  Advanced statistical regression models are available for finding answers to these questions. However, lack of analytic code sharing detailing how the data was manipulated, absence of details about modeling techniques and variables used, and in-sufficient validation of modeling present challenges in understanding how the results could be applicable to one’s practice. STROBE and RECORD guidelines are published to guide the design and reporting of observational studies (OS), particularly, those based on routinely collected health care data. Despite emerging evidence that use of reporting guidelines improve quality of reporting, many journals have still not adopted these guidelines and even when adopted, have not mandated their use. We focus our attention to published OS based on National Cancer Data Base (NCDB), a commonly used database in oncology research, and Journal of Clinical Oncology (JCO), a high-impact journal, and a recent time frame of Jan 2015 to March of 2017.  We checked the 16 publications found to assess how well they followed the 22 criteria specified by STROBE/RECORD guideline. Best-practices especially those recommended by RECORD on code sharing and model validation were followed at low-moderate rate in the range 0-25%. We call for JCO and others to adopt reporting guidelines seriously.  Despite availability of large sample size and a rich array of clinical variables, the results based on NCDB will not progress to clinical practice until we could improve the quality and reproducibility of these studies. 

\item \textbf{Victoria Gamerman}, Boehringer-Ingelheim Pharmaceuticals, Inc. \\
``Focusing on Patients: Going Beyond Rcts'' \\
Steven Edelman, Matthew Capehorn, Anne Belton, Susan Down, Aus Alzaid, Friederike Nagel, Jisoo Lee, William H. Polonsky


Type 2 diabetes (T2D) presents challenges both for physicians, who often have limited time and resources, and for patients, who can experience psychological and behavioural issues. Effective communication between physicians and patients, especially during the early phases of T2D treatment, may lead to improvements in patient self-care and outcomes, which is important considering the clinical benefits associated with achieving good glycaemic control early in the course of T2D.

IntroDia™ – a large cross-national survey in 26 countries – has investigated physician-patient communication during early T2D treatment. The survey was designed in partnership with the International Diabetes Federation and a multidisciplinary advisory board. Around 17,000 participants (physicians and patients with T2D) were surveyed using validated scales and novel questionnaires; these assessed physician-patient communication both at diagnosis and at first prescription of additional oral medication, as well as patient-reported outcomes. 

Overall, findings from IntroDia™ suggest that patient-physician communication at diagnosis of T2D and at add-on may be enhanced by physicians using more collaborative and encouraging – and fewer discouraging – conversation elements, and this may contribute to patients subsequently experiencing greater well-being and managing the disease more effectively.

Methodologies and results from the survey will be highlighted.

\end{itemize}

\subsubsection*{4. Survival Analysis}

\begin{itemize}
\item \textbf{Sangwook Kang}, Yonsei University, Korea \\
``Accelerated Failure Time Modeling via Nonparametric Infinite Scale Mixtures'' \\
Byungtae Seo, Sangwook Kang


A semiparamtric accelerated failure time (AFT) model resembles the usual linear regression model with the response variable being the logarithm of failure times while the random error term is left unspecified. Thus, it is more flexible than parametric AFT models that
assume parametric distributions for the random error term. Estimation for model parameters is typically done through a rank-based procedure, in which the intercept term cannot be directly esitmated. This requires a separate estimation procedure for the intercept, which often leads to unstable estimates. For better estimation of the intercept essential in estimating mean failure times or survival functions, we propose to employ a mixture model approach. To leave the model as 
flexible as possible, we consider nonparametric infinite scale mixtures of normal distributions. An expectation-maximization (EM) method is used to estimate model parameters. Finite sample properties of the proposed estimators are investigated via
an extensive stimulation study. The proposed estimators are illustrated using a real data analysis.

\item \textbf{Daniel Nevo}, Harvard University \\
``Calibration Models for Survival Analysis with Interval-Censored Exposure or Treatment Starting Time'' \\
Daniel Nevo, Tsuyoshi Hamada, Shuji Ogino and Molin Wang


We consider the association of a time-dependent binary treatment or exposure with time-to-event under the proportional hazard model. The exposure value is assumed zero at the beginning of the study and may change to one at any time point. The value of the exposure is observed only in certain time points, and thus its exact value is unknown for some participants, in each risk set. We are motivated by the assessment of post colorectal cancer diagnosis aspiring taking and survival. Naïve and popular methods are potentially biased, especially when the exposure is measured at a small number of time points. We present a class of calibration models that fit a distribution for the time to exposure starting time. Estimates obtained from these models are then incorporated in the partial likelihood in a natural way. We derive asymptotic theory for these methods. Our methodology allows for inclusion of further baseline covariates affecting the initiation time of the exposure of interest.  Certain bias is expected from our methods when the exposure effect is large, and we provide a less-biased alternative using a risk set calibration approach. 

\item \textbf{Bella Vakulenko-Lagun}, Harvard University \\
``Cox Regression for Right-Truncated Data'' \\
Bella Vakulenko-Lagun, Rebecca Betensky, Micha Mandel


Right-truncated survival data arise when observations are sampled retrospectively and
only those who had experienced the event of interest prior to some sampling time are
included in a sample. As a result, the obtained sample is biased, since those who survive
longer have lower probability to be selected.
If the interest is in the nonparametric estimation of the lifetime distribution from right-
truncated data, then this task can be approached by reversing time and transforming the
problem of right-truncation into a well-developed problem of estimation under left truncation. However, when the goal is to explain survival by some covariates, it is unclear how
to interpret results from the reverse time analysis in terms of the forward time effects of
covariates. Other existing methods for the Cox regression under right truncation, although
can be used for testing covariate effect, suffer from an identifiability problem in estimation
or are computationally intensive.
The proposed approach based on the Inverse-Probability-Weighting (IPW) estimating
equations does not have an identifiability problem, it works in a forward time so that covariate effects can be interpreted as usual, it performs better than existing methods for both purposes of testing and estimation, and it is easily implemented using standard software.
Methods are compared in simulations and through an application to real data.

\item \textbf{Jing Qian}, University of Massachusetts-Amherst \\
``Multiple Imputation of Randomly Censored Covariates in Regression Analysis'' \\
Folefac Atem, Jing Qian, Jacqueline Maye, Keith Johnson, Rebecca Betensky


Randomly censored covariates arise frequently in epidemiologic studies. The most commonly used  methods, including complete case and single imputation or substitution, suffer from inefficiency and bias, they make strong parametric assumptions and they consider limit of detection censoring only.  We employ multiple imputation, in conjunction with semi-parametric modeling of the censored covariate, to overcome these shortcomings and to facilitate robust estimation. We develop a multiple imputation approach for randomly censored covariates within the framework of linear and logistic regression models.  We use the nonparametric estimate of the covariate distribution, or the semi-parametric Cox model estimate in the presence of additional covariates in the model.  We evaluate this procedure in simulations, and compare its operating characteristics to those from the complete case analysis and a survival regression approach.  We apply the procedures to an Alzheimer’s study of the association between amyloid positivity and maternal age of onset of dementia.

\end{itemize}

\subsubsection*{5. Complex Data/Network Modeling}

\begin{itemize}
\item \textbf{Xuan Bi}, Yale University \\
``Genome-Wide Mediation Analysis of Psychiatric and Cognitive Traits Through Imaging Phenotypes'' \\
Xuan Bi, Liuqing Yang, Tengfei Li, Baisong Wang, Hongtu Zhu, Heping Zhang


Heritability is well documented for psychiatric disorders and cognitive abilities which are, however, complex, involving both genetic and environmental factors. Hence, it remains challenging to discover which and how genetic variations contribute to such complex traits. In this article, we propose to use mediation analysis to bridge this gap, where neuroimaging phenotypes were utilized as intermediate variables. The Philadelphia Neurodevelopmental Cohort was investigated using genome-wide association studies (GWAS) and mediation analyses. Specifically, 951 participants were included with age ranging from 8 to 21 years. Two hundred and four neuroimaging measures were extracted from structural magnetic resonance imaging scans. GWAS were conducted for each measure to evaluate the SNP-based heritability. Furthermore, mediation analyses were employed to understand the mechanisms in which genetic variants have influence on pathological behaviors implicitly through neuroimaging phenotypes. Our analyses found, rs10494561, located within NMNAT2, to be associated with the severity of the prodromal symptoms of psychosis implicitly, mediated through the volume of the left hemisphere of the superior frontal region. Another SNP rs2285351 was found in the IFT122 gene that may be potentially associated with human spatial orientation ability through the area of the left hemisphere of the isthmuscingulate region.

\item \textbf{Xizhen Cai}, Temple University \\
``Variable Selection for Dynamic Citation Networks'' \\
Xizhen Cai, David Hunter


Recently, survival models like the Cox model are also extended to apply to dynamic network data (Vu et al., 2011b; Perry and Wolfe, 2013), where the observations are dependent. We extend the penalization idea to the Cox model in an egocentric approach to dynamic networks, and select covariates by maximizing the penalized partial likelihood function. Asymptotic properties of both the unpenalized and penalized partial likelihood estimates are developed under certain regularity conditions. We also implement the estimation and test the prediction performance of these estimates in a citation network. Since the covariates are time-varying, the computation cost is high. After variable selection, the model is reduced, which simplifies the calculation for future predictions. Another method to reduce the computational complexity is to use the case-control approximation, in which instead of using all the at-risk nodes in the network, only a subset is sampled to evaluate the partial likelihood function. By using this approximation, the computation time is shortened dramatically, while the prediction performance is still satisfactory in the citation network.

\end{itemize}

\subsubsection*{6. Spatial Analysis of Public Health Data}

\begin{itemize}
\item \textbf{Joshua Warren}, Yale University \\
``a Spatial Method to Estimate Local Vaccine Uptake using Administrative Records'' \\
Joshua Warren, Esra Kurum, Daniel Weinberger


It is necessary to quantify the level of vaccine uptake among a population of interest in order to determine if the introduced vaccine has the desired beneficial impact on human health.  A number of data sources and methods are available to obtain this information at aggregated spatial levels for many vaccines.  However, obtaining an accurate assessment of uptake at more localized spatial scales can be a difficult task due to limitations of regularly collected administrative data.  Vaccine recipients often live in one region while being vaccinated in another, thereby complicating the process of calculating uptake within a region.  We introduce a spatial kernel smoothing method in the Bayesian setting that allows for estimation of local vaccine uptake through the combination of administrative and survey data sources.  The newly developed method is applied to pneumococcal conjugate vaccine uptake data from Brazil in 2013.  Results suggest that the method provides estimates of vaccine uptake at local levels that are in closer agreement to collected survey responses than the standard method that ignores the issue of participant mobility.  The method also provides insight into patterns of mobility of vaccine recipients based on the inclusion of region-specific covariates.         

\item \textbf{Harrison Quick}, Drexel University \\
``Spatiotemporal Trends in Stroke Mortality'' \\
Harrison Quick


Geographic patterns in stroke mortality have been studied as far back as the 1960s, when a region of the southeastern United States became known as the "stroke belt" due to its unusually high rates. While stroke mortality rates are known to increase exponentially with age, an investigation of spatiotemporal trends by age group at the county-level is daunting due to the preponderance of small population sizes and/or few stroke events by age group. Our goal here is two-pronged.  First and foremost, we harness the power of a complex, nonseparable multivariate space-time model which borrows strength across space, time, and age group to obtain reliable estimates of yearly county-level mortality rates from US counties between 1973 and 2013 for those aged 65+. Second, we outline how the results of this model fit can be used to generate high-quality synthetic data for public use that preserve data confidentiality without sacrificing data utility.

\item \textbf{Chanmin Kim}, Harvard University \\
``Public Health Impact of Pollutant Emissions'' \\
Corwin Zigler, Christine Choirat


Pollutant emissions from coal burning power plants have been deemed to adversely impact ambient air quality and public health conditions. Over the last few decades, many air quality control strategies targeting emissions have been adopted at the U.S. power plants. Despite noticeable reduction in emissions and the improvement of air quality since the Clean Air Act (CAA) became the law, the public-health benefits from changes in emissions have not been widely evaluated yet. In terms of the chain of accountability, the link between pollutant emissions and public health conditions with counting for changes in ambient air quality, we provide the first epidemiological assessment of the health effect of specific pollutant emission (SO2) that is mediated through change in the ambient air quality. Especially, we pursue the link from SO2 emissions from coal-fired power plants (intervention) to ambient PM2.5 concentrations (mediator) estimated for each zip code and from ambient PM2.5 to cardiovascular- and respiratory-hospitalization and all causes mortality (outcomes). The main linkage is based on the HYSPLIT model developed by the National Oceanic and Atmospheric Administration (NOAA), which simulates air mass trajectories from coal-fired power plants. To draw causality in the observational data, we use the potential outcomes framework with direct adjustment for confounding variables in the regression model. Then, we use a newly-developed Bayesian nonparametric method to provide flexible models to the observed data in two analyses: principal stratification analysis and mediation analysis. Both analyses are anchored to the same observed data model and used as the means to quantify the effects through two causal pathways: the extent to which SO2 emissions affect public health outcomes that is attributable to changes in ambient PM2.5 and the extent to which SO2 emissions directly affect public health outcomes.

\item \textbf{Gavino Puggioni}, University of Rhode Island \\
``Spatiotemporal Analysis of Vector-Borne Disease Risk'' \\
Janelle Couret, Emily Serman, Ali Akanda, Howard Ginsberg


This work presents the first comprehensive spatio-temporal analysis that links reported and suspected cases of Dengue (recorded monthly in Puerto Rico from 1990 to 2015 in 76 municipalities) with weather variables collected at 34 stations and land use satellite data. Dengue and Zika are mosquito-borne tropical diseases, reported with increasing rates in the last decade. Early warning systems help in predicting outbreaks and allow public health decision-makers to implement preventive measures. Several factors have been linked to the increase in reported cases: changes in temperature, precipitation, urbanization, and other spatial variables.  Several space-time CAR specifications are implemented in a Bayesian framework to assess the relative risk of these factors, as well as to set a predictive framework. The modeling strategy involves a two stage approach to account for the different spatial supports of predictors and response. 

\end{itemize}

\subsubsection*{7. Network Data Analysis}

\begin{itemize}
\item \textbf{Jp Onnela}, Harvard University \\
``Parameter Inference and Model Selection for Mechanistic Network Models'' \\
JP Onnela, Antonietta Mira


There are (at least) two prominent paradigms to the modeling of networks, which might be called the statistical approach and the mechanistic approach. In the statistical approach, one describes a model that specifies the likelihood of observing a given network, i.e., these are probabilistic models of data that happen to arrive in the shape of a network. In the mechanistic approach, one specifies a set of domain-specific mechanistic rules, based on scientific understanding of the problem, that are used to grow or evolve the network over time. Both modeling approaches provide distinct angles and advantages to our understanding of complex systems. I propose to discuss two interrelated topics. First, I will present a framework to identify the essential network properties associated with mechanistic network models. The joint distribution of these properties determines whether it is feasible to model the given networks using a specific statistical framework. Second, I will discuss a general framework for both parameter inference and model selection for mechanistic network models. In the former, a mechanistic network model might incorporate some parameter whose value is not fixed and must therefore be learned from data. In the related problem of model selection, the goal is to learn our degree of belief that any of the candidate mechanistic models was used to generate the network. The proposed approach can quantify the probability that any given model in the collection generated the data, and this approach can therefore be used to select the most likely model from among the collection of candidate models.

\item \textbf{Xinran Li}, Harvard University \\
``Randomization Inference for Peer Effects'' \\
Xinran Li, Peng Ding, Qian Lin, Dawei Yang and Jun S. Liu


Many previous causal inference studies required no interference among units, that is, the potential outcomes of a unit do not depend on the treatments of other units. This no-interference assumption, however, becomes unreasonable when units are partitioned into groups and they interact with other units  within groups. In a motivating education example from Peking University, students are admitted either through the college entrance exam (also known as Gaokao), or recommendation (often based on Olympiads in various subjects). Right after entering college, students are randomly assigned to different dorms, each of which hosts four students. Because students within the same dorm live together almost every day and they interact with each other intensively, it is very likely that peer effects exist and the no-interference assumption is violated. More importantly, understanding peer effects among students gives useful guidance for future roommate assignment to improve the overall performances of the students. Methodologically, we define peer effects in terms of potential outcomes, and propose a randomization-based inference framework to study peer effects in general settings with arbitrary numbers of peers and arbitrary numbers of peer types. Our inferential procedure does not require any parametric modeling assumptions on the outcome distributions. Our analysis of the data set from Peking University gives useful practical guidance for policy makers.

\item \textbf{Vishesh Karwa}, Harvard University \\
``Estimating Average Treatment Effects under Interference: Modes of Failure and Solutions'' \\
Vishesh Karwa, Edo Airoldi


Estimating average treatment effects in the presence of network interference has recently gained a lot of attention. It is well known that the classical versions of average treatment effects are no longer well defined when there is interference. We will investigate the issues that arise in defining and estimating average causal effects when there is arbitrary interference. We posit models for specifying various forms of interference through the concept of an \emph{exposure} neighborhood and develop a linear non-parametric representation of potential outcomes. Focusing on unbiased estimation (with respect to the randomization distribution), we will study two types of average causal effects. We will examine the pitfalls of ignoring interference and the consequences of using classical designs and estimators for estimating average causal effects. The classical difference-of-means estimators can have arbitrary bias. The nature and source of bias depends on the form of interference, which is unknown in general. On the other hand, Horvitz-Thompson estimators are unbiased as along as the correct weights are used; these weights depend on the form of interference. We also show that the H-T estimators are in-admissible for a large class of designs.

\end{itemize}

\subsubsection*{8. Statistical Approaches to Data Modeling and Analysis}

\begin{itemize}
\item \textbf{Patrick Flaherty}, University of Massachusetts-Amherst \\
``a Deterministic Global Optimization Method for Variational Inference'' \\
Hachem Saddiki, Andrew C. Trapp, Patrick Flaherty


Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees.
However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum.
Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood.
We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM).
We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound.
We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.

\item \textbf{Matthias Steinruecken}, University of Massachusetts-Amherst \\
``Unraveling the Demographic History of Modern Humans using Full-Genome Sequencing Data'' \\
Matthias Steinruecken


Contemporary and ancient demographic structure in human populations has shaped the genomic variation observed in modern humans, and severely affected the distribution of functional and disease related genetic variation. Using next-generation sequencing technologies, researchers gather increasing amounts of genomic sequencing data for large samples in many different human population groups. These datasets present unprecedented opportunities to study genomic variation in complex demographic scenarios, and this area has received a lot of attention in recent years.


In this talk, I will present a method for the inference of demographic histories from full-genome sequencing data of multiple individuals developed by me and my collaborators. I will apply this method to a genomic dataset of Native American individuals to unravel the ancient demographic events underlying the peopling of the Americas. Morevoer, I will discuss a novel method for demographic inference that has the potential to improve inference especially in the recent past, which is of particular importance in the context of complex genetic diseases in humans.

\item \textbf{Evan L. Ray}, University of Massachusetts, Amherst \\
``Feature-Weighted Ensembles for Probabilistic Time-Series Forecasts'' \\
Evan L. Ray, Nicholas G. Reich


Accurate and reliable predictions of infectious disease incidence are important for public health decision makers planning resource allocation and interventions designed to prevent or reduce disease transmission. Ensemble prediction methods, which combine predictions from multiple ``component" models, have recorded superior performance in a variety of tasks from weather prediction to product recommendation; however, applications of ensemble methods in the context of predicting infectious disease have been limited. We considered a range of ensemble methods that each form a predictive density for a target of interest as a weighted sum of the predictive densities from several component models. In the simplest case, equal weight is assigned to each component model; in the most complex case, the weights vary with multiple observed features such as recent observations of disease incidence and the time of the year when predictions are made. We applied these methods to predict measures of influenza season timing and severity in the United States, both at the national and regional levels, using three component models. We trained the models on retrospective predictions from 14 seasons (1997/1998 - 2010/2011) and evaluated each model's prospective, out-of-sample performance in the five subsequent influenza seasons. In this test phase, the ensemble methods showed overall performance that was similar to the best of the component models, but offered more consistent performance across seasons than the component models. Ensemble methods offer the potential to deliver more reliable infectious disease predictions to public health decision makers.

\item \textbf{Daeyoung Kim}, University of Massachusetts-Amherst \\
``Confidence Distribution Sampling and Its Application'' \\
Daeyoung Kim


Inference functions, such as the likelihood, are widely used throughout statistics. They have the virtue of providing methods for point estimation, set estimation, and hypothesis testing.
Researchers often perform  inferences based on the asymptotic theory.  
But, the asymptotic-based inferences may not be reliable if the amount of available information is not large relative to the number of parameters. 
To address this long-standing problem, we have developed the methodologies to assess the adequacy of using the asymptotic
theory for finite-sample inference.  They are based on confidence distribution sampling and volumetric error analysis.

\end{itemize}

\subsubsection*{9. Social Networks and Causal Inference}

\begin{itemize}
\item \textbf{Dean Eckles}, Mit \\
``Estimating Peer Effects in Networks with Peer Encouragement Designs'' \\
Dean Eckles, Rene Kizilcec, Eytan Bakshy


Peer effects, in which the behavior of an individual is affected by the behavior of their peers, are central to social science. Because peer effects are often confounded with homophily and common external causes, recent work has used randomized experiments to estimate effects of specific peer behaviors. These experiments have often relied on the experimenter being able to randomly modulate mechanisms by which peer behavior is transmitted to a focal individual. We describe experimental designs that instead randomly assign individuals’ peers to encouragements to behaviors that directly affect those individuals. We illustrate this method with a large peer encouragement design on Facebook for estimating the effects of receiving feedback from peers on posts shared by focal individuals. We find evidence for substantial effects of receiving marginal feedback on multiple behaviors, including giving feedback to others and continued posting. These findings provide experimental evidence for the role of behaviors directed at specific individuals in the adoption and continued use of communication technologies. In comparison, observational estimates differ substantially, both underestimating and overestimating effects, suggesting that researchers and policy makers should be cautious in relying on them.

\item \textbf{Hyunseung Kang}, University of Wisconsin Madison \\
``Peer Encouragement Designs in Causal Inference with Partial Interference and Identification of Local Average Network Effects'' \\
Hyunseung Kang, Guido Imbens


In non-network settings, encouragement designs have been widely used to analyze causal effects of a treatment, policy, or intervention on an outcome of interest when randomizing the treatment was considered impractical or when compliance to treatment cannot be perfectly enforced. Unfortunately, such questions related to treatment compliance have received less attention in network settings and the most well-studied experimental design in networks, the two-stage randomization design, requires perfect compliance with treatment. The paper proposes a new experimental design called peer encouragement design to study network treatment effects when enforcing treatment randomization is not feasible. The key idea in peer encouragement design is the idea of personalized  encouragement, which allows point-identification of familiar estimands in the encouragement design literature. The paper also defines new causal estimands, local average network effects, that can be identified under the new design and analyzes the effect of non-compliance behavior in randomized experiments on networks.

This is joint work with Guido Imbens (Stanford)

\item \textbf{Alexander Volfovsky}, Duke University \\
``Causal Inference in the Presence of Networks: Randomization and Observation'' \\
Alexander Volfovsky


Much of classical causal analysis relies on notions of independence. However, modern datasets on disease prevalence, social development, online advertising and business transactions come equipped with information on a network that links the units together, rendering these notions implausible. When designing randomized experiments, scientists must control for network interference and homophily in order to guarantee the theoretical properties of their estimators. Studying the direct treatment effect in networks, we describe a new class of randomizations that can guarantee unbiasedness and control the variance of the estimator. In situations where an experiment cannot be performed, causal analysis requires the use of matching techniques in order to protect against bias due to a lack of balance between treated and control units. We provide examples of the complications that arise when information about the network is disregarded and develop a matching technique that extends classical propensity scores to the realm of networks.

\item \textbf{Daniel Sussman}, Boston University \\
``Ptimal Unbiased Estimation of Causal Effects under Network Interference'' \\
Daniel Sussman, Edo Airoldi


From a causal inference perspective, the typical assumption of no interference becomes untenable in experiments in a social context. In many instances, however, the patterns of interference may be informed by the observation of network connections among the units of analysis. We develop elements of optimal estimation theory for causal effects by leveraging an observed network. Considering the class of linear unbiased estimators of the average direct treatment effect under various exclusion restrictions for the potential outcomes, we offer analytical insights on the weights that lead to minimum integrated variance estimators. These estimators offer superior performance to previously proposed estimators and we seek to develop complementary variance estimators for these estimates based on similar principles.

\end{itemize}

\subsubsection*{10. Statistical Innovations in Genomics}

\begin{itemize}
\item \textbf{Hongkai Ji}, Johns Hopkins University \\
``Single-Cell Rna-Seq Analysis by Spanning Trees'' \\
Zhicheng Ji, Hongkai Ji


When analyzing single-cell RNA-seq data, constructing a pseudo-temporal path to order cells based on the gradual transition of their transcriptomes is a useful way to study gene expression dynamics in a heterogeneous cell population. We present TSCAN, a new tool for single cell pseudo-time analysis. TSCAN uses a cluster-based minimum spanning tree (MST) approach to order cells. Cells are first grouped into clusters and an MST is then constructed to connect cluster centers. Pseudo-time is obtained by projecting each cell onto the tree, and the ordered sequence of cells can be used to study dynamic changes of gene expression along the pseudo-time. Clustering cells before MST construction reduces the complexity of the tree space. This often leads to improved cell ordering. It also allows users to conveniently adjust the ordering based on prior knowledge. TSCAN has a graphical user interface (GUI) to support data visualization and user interaction. Furthermore, quantitative measures are developed to objectively evaluate and compare different pseudo-time reconstruction methods.

\end{itemize}

\subsubsection*{11. Recent Developments on High-Dimensional Statistics and Regularized Estimation}

\begin{itemize}
\item \textbf{Ethan Fang}, Pennsylvania State University-Main Campus \\
``Blessing of Massive Scale: Spatial Graphical Model  Estimation with a Total Cardinality Constraint Approach'' \\
Ethan Fang, Han Liu, Mengdi Wang


We consider the problem of estimating high dimensional spatial graphical models with a total cardinality constraint. Though this problem is highly nonconvex, we show that its primal-dual gap diminishes linearly with the dimensionality and provide a convex geometry justification of this ``blessing of massive scale" phenomenon. Motivated by this result, we propose an efficient algorithm to solve the dual problem (which is concave) and prove that the solution achieves optimal statistical properties. Extensive numerical results are also provided.

\item \textbf{Cheng Yong Tang}, Temple University \\
``Sufficient Dimension Reduction with Missing Data'' \\
Yuexiao Dong, Cheng Yong Tang, Qi Xia


Inverse regressions constitute a class of sufficient dimension reduction methods targeting at estimating the central space by regression-type approaches implemented inversely on the predictors and the responses. The most representative approach in this family is the seminal Sliced Inverse Regression (SIR) approach proposed by Li (1991). In this study, we first show that missing responses generally affect the validity of the inverse regressions under the scheme of the so-called missing at random, in the sense that the resulting estimations for the central space can be biased if data with missing responses are simply ignored.  We then propose two simple and effective adjustments for missing responses that guarantees the validity of the inverse regressions. The proposed methods share the essence and simplicity of the inverse regressions. We demonstrate the performance of the proposed inverse regressions for dealing with missing responses by numerical and theoretical analyses. 

\item \textbf{Ting Zhang}, Boston University \\
``a Thresholding-Based Prewhitened Long-Run Variance Estimator and Its Dependence-Oracle Property'' \\
Ting Zhang


Statistical inference of time series data routinely relies on the estimation of long-run variances, defined as the sum of autocovariances of all orders. The current paper considers a new class of long-run variance estimators, which first soaks up the dependence by a decision-based prewhitening filter, then regularizes autocorrelations of the resulting residual process by thresholding, and finally recolors back to obtain an estimator of the original process. Under mild regularity conditions, we prove that the proposed estimator (i) consistently estimates the long-run variance; (ii) achieves the parametric convergence rate when the underlying process has a sparse dependence structure as in finite-order moving average models; and (iii) enjoys the dependence-oracle property in the sense that it will automatically reduce to the sample variance if the data are actually independent. Monte Carlo simulations are conducted to examine its finite-sample performance and make comparisons with existing estimators.

\item \textbf{Sahand Negahban}, Yale University \\
``On Approximation Guarantees for Greedy Low Rank Optimization'' \\
Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, and Sahand Negahban


We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions of restricted strong convexity and smoothness. Our novel analysis also uncovers previously unknown connections between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of corresponding approximation bounds in submodular maximization. Additionally, we also provide statistical recovery guarantees.

\end{itemize}

\subsubsection*{12. Subgroup Analysis}

\begin{itemize}
\item \textbf{Wai-Ki Yip}, Foundation Medicine, Inc. \\
``Sr. Biostatistician'' \\
Wai-Ki Yip, Marco Bonetti, Ann Lazar, William Barcella, Victoria Xin Wang, Chip Cole, Rich Gelber


The Subpopulation Treatment Effect Pattern Plot is a visual and statistical technique to explore patterns of treatment effects across values of a continuously measured covariate such as a biomarker measurement. Originally developed specifically for investigation of survival outcomes, it has been extended to continuous, binary and count outcomes.  This talk will focus on the development of this extension – what are the outcomes, the permutation statistics and the Type 1 error, the power of the test, comparison with other methods, and the software.  Then, a motivating example of how it is applied to analyze data from the Aspirin/Folate Polyp Prevention Study will be presented. 
A quick summary of recent research development in STEPP will be presented at the end.

\item \textbf{Yanxun Xu}, Johns Hopkins University \\
``a Nonparametric Bayesian Basket Trial Design'' \\
Yanxun Xu, Peter Mueller, Apostolia Tsimberidou, Donald Berry


Targeted therapies on the basis of genomic aberrations analysis of the tumor have shown promising results in cancer prognosis and treatment. Regardless of tumor type, trials that match patients to targeted therapies for their particular genomic aberrations have become a mainstream direction of therapeutic management of patients with cancer. Therefore, finding the subpopulation of patients who can most benefit from an aberration-specific targeted therapy across multiple cancer types is important. We propose an adaptive Bayesian clinical trial design for patient allocation and subpopulation identification. We start with a decision theoretic approach, including a utility function and a probability model across all
possible subpopulation models. The main features of the proposed design and population finding methods are that we allow for variable sets of covariates to be recorded by different patients, adjust for missing data, allow high order interactions of covariates, and the adaptive allocation of each patient to treatment arms using the posterior predictive probability of which arm is best for each patient. The new method is demonstrated via extensive simulation studies.

\item \textbf{Jared Huling}, University of Wisconsin-Madison \\
``Heterogeneity of Intervention Effects and Subgroup Identification Based on Longitudinal Outcomes'' \\
Jared Huling, Menggang Yu, Maureen Smith


With the growing health costs in the United States, the need to improve the efficiency and efficacy
of care has become increasingly urgent. There has been great interest in developing interventions to
effectively coordinate the typically fragmented care of  patients with many comorbidities. Evaluation
of such interventions is often challenging given their long-term nature and their differential effectiveness among different patients. Given this and the resource intensiveness of care coordination interventions, there is significant interest in identifying which patients may benefit the most from care coordination. Identification of patients which benefit from a particular intervention can be accomplished by modeling covariates which modify the intervention effect. In this work we extend the interaction modeling framework of Tian, et al. (2014) and Chen, et al. (2017) to handle long-term interventions whose effects are expected to change smoothly over time. We  allow interaction effects to vary over time and encourage these effects to be more similar over time by utilizing a fused lasso penalty. Our approach allows for flexibility in modeling temporal effects while also borrowing strength in estimating these effects over time. We use our approach to identify a subgroup of patients who benefit from a complex case management intervention in a large hospital system.

\end{itemize}

